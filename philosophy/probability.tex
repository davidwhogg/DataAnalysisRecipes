% This file is part of the Data Analysis Recipes project.
% Copyright 2012 David W. Hogg (NYU)

% to-do items
% -----------
% - explain the over-conditioning problem with p(a|b,c) p(b|a,c).

% style notes
% -----------
% - when at the end of the sentence, put the \endnote AFTER the period
% - when at the end of a phrase, put the \endnote BEFORE the comma or parens
% - make sure the endnotes can be read on their own, outside of context
% - careful with the words ``error'', ``uncertainty''
% - careful with the words ``probability'', ``frequency'', ``likelihood''
% - use () for function arguments, and [] for grouping/precedence
% - define macros; remember 1, 2, infinity
%   - (check out my awesome \given macro)
% - put new terms in \emph{}, put only referred-to words in quotation marks.
% - do in-text itemized lists with \textsl{(a)}~ and so on.

\documentclass[12pt,twoside]{article}
\usepackage{amssymb,amsmath,mathrsfs,../hogg_endnotes,natbib}
\usepackage{float,graphicx}

\setlength{\emergencystretch}{2em}%No overflow

\newcommand{\notenglish}[1]{\textsl{#1}}
\newcommand{\aposteriori}{\notenglish{a~posteriori}}
\newcommand{\apriori}{\notenglish{a~priori}}
\newcommand{\adhoc}{\notenglish{ad~hoc}}
\newcommand{\etal}{\notenglish{et al.}}

\newcommand{\documentname}{document}
\newcommand{\documentnames}{\documentname s}
\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}
\newcommand{\figurenames}{\figurename s}
\newcommand{\problemname}{Exercise}
\newcommand{\problemnames}{\problemname s}
\newcommand{\solutionname}{Solution}
\newcommand{\notename}{note}

\newcommand{\note}[1]{\endnote{#1}}
\def\enotesize{\normalsize}
\renewcommand{\thefootnote}{\fnsymbol{footnote}} % the ONE footnote needs this

\newcounter{problem}
\newenvironment{problem}{\paragraph{\problemname~\theproblem:}\refstepcounter{problem}}{}
\newcommand{\affil}[1]{{\footnotesize\textsl{#1}}}

% matrix stuff
\newcommand{\mmatrix}[1]{\boldsymbol{#1}}
\newcommand{\inverse}[1]{{#1}^{-1}}
\newcommand{\transpose}[1]{{#1}^{\scriptscriptstyle \top}}
% parameter vectors
\newcommand{\parametervector}[1]{\mmatrix{#1}}
\newcommand{\pvtheta}{\parametervector{\theta}}
% set stuff
\newcommand{\setofall}[3]{\{{#1}\}_{{#2}}^{{#3}}}
\newcommand{\allq}{\setofall{q_i}{i=1}{N}}
% other random multiply used math symbols
\newcommand{\dd}{\mathrm{d}}
\newcommand{\given}{\,|\,}

% header stuff
\renewcommand{\MakeUppercase}[1]{#1}
\pagestyle{myheadings}
\renewcommand{\sectionmark}[1]{\markright{\thesection.~#1}}
\markboth{Probability calculus for inference}{Introduction}

\begin{document}
\thispagestyle{plain}\raggedbottom
\section*{DRAFT 2012-03-18:\ Data analysis recipes:\ \\
  Probability calculus for inference\footnotemark}

\footnotetext{%
  The \notename s begin on page~\pageref{note:first}, including the
  license\note{\label{note:first}%
    Copyright 2012 David W. Hogg (NYU).  You may copy and distribute this
    document provided that you make no changes to it whatsoever.}
  and the acknowledgements\note{%
    It is a pleasure to thank
      [insert names here]
    for discussions and comments that shaped these ideas.  This
    research was partially supported by the US National Aeronautics
    and Space Administration and National Science Foundation.}}

\noindent
David~W.~Hogg\\
\affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics, New York University}\\
\affil{Max-Planck-Institut f\"ur Astronomie, Heidelberg}
%% \\[1ex]
%% A. Nother Author
%% \affil{Center~for~Cosmology~and~Particle~Physics, Department~of~Physics, New York University}

\begin{abstract}
I review the rules by which probability distribution functions can
(and cannot) be combined. I connect these rules to the operations
performed in probabilistic data analysis.  Dimensional analysis is
emphasized as a valuable tool for helping to construct non-wrong
probabilistic statements.
\end{abstract}

\noindent
When constructing the plan or basis for a probabilistic inference---a
data analysis making use of likelihoods and also prior PDFs and
posterior PDFs and also marginalization of nuisance parameters---the
options are \emph{incredibly strongly constrained} by the simple rules
of probability calculus.  That is, there are only certain ways that
probability distribution functions (``PDFs'' in what follows)
\emph{can} be combined to make new PDFs, compute expectation values,
or make other kinds of non-wrong statements.  For this reason, it
behooves the data analyst to have good familiarity and facility with
these rules.

Formally, probability calculus is extremely simple.  However, it is not
always part of a physicist's (or biologist's or chemist's or
economist's) education; so I am going to give a very fast review right
here.  The key idea I want to convey is that if you think about the
units or dimensions of the things you are using, you almost never go
wrong.

For space and specificity---and because it is most useful for most
problems I encounter---I will focus on continuous variables (think
``model parameters'' for the purposes of inference) rather than
binary, integer, or discrete parameters, although I might say one or
two things here or there.  I also won't always be specific about the
domain of variables (for example whether a variable $a$ is defined on
$0<a<1$ or $0<a<\infty$ or $-\infty<a<\infty$); the limits of
integrals---all of which are definite---will usually be
implicit.\note{In my world, \emph{all integrals are definite
    integrals}.  The integral is never just the anti-derivative, it is
  always a definite, finite, area or volume or measure.  That has to
  do with the fact, somehow, that the rules of probability calculus
  are an application of the rules of measure theory.}

\section{Generalities}

Probability distribution functions have units or dimensions.  Don't
ignore them.  For example, if you have a continuous parameter $a$, and
a PDF $p(a)$ for $a$, it must obey the normalization condition
\begin{eqnarray}\displaystyle
1 &=& \int p(a)\,\dd a
\quad ,
\end{eqnarray}
where the limits of the integral should be thought of as going over
the entire domain of $a$.  This is almost the \emph{definition} of a
PDF, from my (pragmatic, informal) point of view.  This normalization
condition shows that $p(a)$ has units of $a^{-1}$.  Nothing else would
integrate properly to a dimensionless result.  Even if $a$ is a
multi-dimensional vector or list or tensor or field or even point in
function space, the PDF must have units of $a^{-1}$.

In the multi-dimensional case, the units of $a^{-1}$ are found by
taking the product of all the units of all the dimensions.  So, for
example, if $a$ is a six-dimensional phase-space position in
three-dimensional space (three cartesian position components measured
in m and three cartesian momentum components measured in
kg\,m\,s$^{-1}$), the units of $p(a)$ would be
kg$^{-3}$\,m$^{-6}$\,s$^3$.

Most problems we will encounter will have multiple parameters; even if
we \emph{condition} $p(a)$ on some particular value of another
parameter $b$, that is, ask for the PDF for $a$ \emph{given} that $b$
has a particular, known value to make $p(a \given b)$ (read ``the PDF
for $a$ given $b$''), it must obey the same marginalization
\begin{eqnarray}\displaystyle
1 &=& \int p(a \given b)\,\dd a
\quad ,
\end{eqnarray}
but you can \emph{absolutely never do} the integral
\begin{eqnarray}\displaystyle
\mbox{\textbf{wrong:}} & & \int p(a \given b)\,\dd b
\end{eqnarray}
because that integral would have units of $a^{-1}\,b$, which is (for
our purposes) absurd.  Despite its absurdity, this integral has been
performed many times in error in the literature of data analyses.

If you have a probability distribution for two things (``the PDF for
$a$ and $b$''), you can always factorize it into two distributions,
one for $a$, and one for $b$ given $a$ or the other way around:
\begin{eqnarray}\displaystyle
p(a, b) &=& p(a)\,p(b \given a)
\\
p(a, b) &=& p(a \given b)\,p(b)
\quad ,
\end{eqnarray}
these two factorizations, taken together lead to what is sometimes
called ``Bayes's theorem'' but is just a simple consequence of the
factorization:
\begin{eqnarray}\displaystyle
p(a \given b) &=& \frac{p(b \given a)\,p(a)}{p(b)}
\quad ,
\end{eqnarray}
where the ``divide by $p(b)$'' aspect of that gives many philosophers
and mathematicians the chills (though certainly not me).\note{Division
  by zero is a huge danger---in principle---when applying Bayes's
  theorem.  In practice, if there is support for the model in your
  data, or support for the data in your model, you don't hit any
  zeros.}  Conditional probabilities factor just the same as
unconditional ones (and many will tell you that there is no such thing
as an unconditional probability\note{There are no unconditional
  probabilities!  This is because whenever \emph{in practice} you
  calculate a probability or a PDF, you are always making strong
  assumptions.  Your probabilities are all conditioned on these
  assumptions.}); they factor like this:
\begin{eqnarray}\displaystyle
p(a, b \given c) &=& p(a \given c)\,p(b \given a, c)
\\
p(a, b \given c) &=& p(a \given b, c)\,p(b \given c)
\\
p(a \given b, c) &=& \frac{p(b \given a, c)\,p(a \given c)}{p(b \given c)}
\quad ,
\end{eqnarray}
where the condition $c$ must be carried through all the terms; the
whole right-hand side must be conditioned on $c$ if the left-hand side
is.  Again, there was Bayes's theorem, and you can see its role in
conversions of one kind of conditional probability into another.  For
technical reasons, I usually write Bayes's theorem like this:
\begin{eqnarray}\displaystyle\label{eq:bayes}
p(a \given b, c) &=& \frac{1}{Z}\,p(b \given a, c)\,p(a \given c)
\\
Z &\equiv& \int p(b \given a, c)\,p(a \given c)\,\dd a
\quad .
\end{eqnarray}

Here are things you \emph{can't} do:
\begin{eqnarray}\displaystyle
\mbox{\textbf{wrong:}} & & p(a \given b, c)\,p(b \given a, c)
\\
\mbox{\textbf{wrong:}} & & p(a \given b, c)\,p(a \given c)
\quad;
\end{eqnarray}
the first over-conditions (it is not a factorization of anything
possible) and the second has units of $a^{-2}$, which is absurd (for
our purposes).  Know these and \emph{don't do them}.

One important and confusing point about all this: The terminology used
throughout this \documentname\ \emph{enormously overloads} the symbol
$p(\cdot)$.  That is, we are using, in each line of this discussion,
the function $p(\cdot)$ to mean something different; it's meaning is
set by the letters used in its arguments.  That is a nomenclatural
abomination.\note{Serious, non-ambiguous mathematicians often
  distinguish between the name of the variable and the name of a draw
  from the probability distribution for the variable, and then instead
  of $p(a \given b, c)$ they can write things like $p_{A,B=b,C=c}(a)$,
  which are unambiguous (or far less ambiguous).  This permits, for
  example, another thing $q$ to be drawn from the same distribution as
  $a$ by a notation like $p_{A,B=b,C=c}(q)$.  In our (very bad)
  notation $p(q \given b, c)$ would in general be different from $p(a
  \given b, c)$ because we take the meaning of the function from the
  names of the argument variables.  See why that is very bad?.  Some
  explicit subscripting policy seems like much better practice and we
  should probably all adopt something like it, though I won't
  here.\par Another good idea is to make the probability equations be
  about statements, so instead of writing $p(a \given b, c)$ you write
  $p(A=a \given B=b, C=c)$.  This is unambiguous---you can write
  useful terms like $p(A=q \given B=b, C=c)$ to deal with the $a$, $q$
  problem---but it makes the equations big and long.}  I apologize,
but it is so standard in our business I won't change.

``Measure theory'' (for me, anyway\note{Have you noticed that I am
  \emph{not} a mathematician?}) is the theory of things in which you
can do integrals.  You can ``integrate out'' variables you want to get
rid of (or, in what follows, \emph{not} infer) by integrals that look
like
\begin{eqnarray}\displaystyle\label{eq:marginalize}
p(a \given c) &=& \int p(a \given b, c)\,p(b \given c)\,\dd b
\quad ,
\end{eqnarray}
where again the integrals go over the entire domain of $b$ in each
case, and again if the left-hand side is conditioned on $c$, then
everything on the right-hand side must be also.  This equation is a
natural consequence of the things written above and dimensional
analysis.  Recall that because $b$ is some kind of arbitrary, possibly
very high-dimensional mathematical object, these integrals can be
extremely daunting in practice (see below).  Sometimes equations like
(\ref{eq:marginalize}) can be written
\begin{eqnarray}\displaystyle
p(a \given c) &=& \int p(a \given b)\,p(b \given c)\,\dd b
\quad ,
\end{eqnarray}
where the dependence of $p(a \given b)$ on $c$ has been dropped.  This
is only permitted if it \emph{happens to be the case} that $p(a \given
b, c)$ doesn't, in practice, depend on $c$.  The dependence on $c$ is
really there (in some sense), it just might be trivial or null.

In rare cases, you can get factorizations that look like this:
\begin{eqnarray}\displaystyle
p(a, b \given c) &=& p(a \given c)\,p(b \given c)
\quad ;
\end{eqnarray}
this factorization doesn't have the PDF for $a$ depend on $b$ or vice
versa.  When this happens---and it is rare---it says that $a$ and $b$
are ``independent'' (at least conditional on $c$).\note{The word
  ``independent'' has many meanings in different contexts of
  mathematics and probability; I will avoid it in what follows, except
  in context of independently drawn data points in a generative model
  of data.  I prefer the word ``separable'' for this situation,
  because I think it is less ambiguous.}  In many cases of data
analysis, in models of data sets, we often have a large number of data
$a_n$, indexed by $n$, each of which is not only independent in this
sense, but also drawn from the same distribution function.  Data of
this form are called ``independent and identically distributed'' or
``iid'' in the literature.  If you have a set of $N$ independently
drawn data $a_n$, each drawn from a probability distribution $p(a_n
\given c)$, then the probability of the full data set is simply the
product of the individual data-point probabilities:
\begin{eqnarray}\displaystyle
p(\setofall{a_n}{n=1}{N} \given c) &=& \prod_{n=1}^N p(a_n \given c)
\quad ;
\end{eqnarray}
this is the definition of ``independent data points''.

I am writing here mainly about continuous variables, but one thing
that comes up frequently in data analysis are ``mixture models'' in
which data are produced by two (or more) qualitatively different
processes (some data are good, and some are bad, for example) that
have different relative probabilities.  When a variable ($b$, say) is
discrete, the marginalization integral corresponding to
\equationname~(\ref{eq:marginalize}) becomes a sum
\begin{eqnarray}\displaystyle
p(a \given c) &=& \sum_b p(a \given b, c)\,p(b \given c)
\quad ,
\end{eqnarray}
and the normalization of $p(b \given c)$ becomes
\begin{eqnarray}\displaystyle
1 &=& \sum_b p(b \given c)
\quad ;
\end{eqnarray}
in both sums, the sum is implicitly over the (small number of)
possible states of $b$.

If you have a conditional PDF for $a$ like $p(a \given c)$ and you
want to know the expectation value $E(a \given c)$ of $a$ under this
PDF (which would be, for example, something like the mean value of $a$
you would get if you drew many draws from the conditional PDF), you
just integrate
\begin{eqnarray}\displaystyle
E(a \given c) &=& \int a\,p(a \given c)\,\dd a
\quad .
\end{eqnarray}
This generalizes to any function $f(a)$ of $a$:
\begin{eqnarray}\displaystyle
E(f \given c) &=& \int f(a)\,p(a \given c)\,\dd a
\quad .
\end{eqnarray}
You can see the marginalization integral (\ref{eq:marginalize}) that
converts $p(a \given b, c)$ into $p(a \given c)$ as providing the
\emph{expectation value} of $p(a \given b, c)$ under the conditional
PDF $p(b \given c)$.  That's deep and relevant for what follows.

\begin{problem}
You have conditional PDFs $p(a \given d)$, $p(b \given a, d)$, and
$p(c \given a, b, d)$.  Write expressions for $p(a, b \given d)$, $p(b
\given d)$, and $p(a \given c, d)$.
\end{problem}

\begin{problem}
You have conditional PDFs $p(a \given b, c)$ and $p(a \given c)$
expressed or computable for any values of $a$, $b$, and $c$.  You are not
permitted to multiply these together, of course.  But can you use them
to construct the conditional PDF $p(b \given a, c)$ or $p(b \given c)$?
\end{problem}

\begin{problem}
You have conditional PDFs $p(a \given c)$ and $p(b \given c)$
expressed or computable for any values of $a$, $b$, and $c$.  Can you
use them to construct the conditional PDF $p(a \given b, c)$?
\end{problem}

\begin{problem}
You have a function $g(b)$ that is a function only of $b$.  You have
conditional PDFs $p(a \given c)$ and $p(b \given a, c)$.  What is the
expectation value $E(g \given c)$ for $g$ conditional on $c$ but
\emph{not} conditional on $a$?
\end{problem}

\begin{problem}
Take the integral on the right-hand side of
\equationname~(\ref{eq:marginalize}) and replace the ``$\dd b$'' with
a ``$\dd a$''.  Is it permissible to do this integral?  Why or why
not?  If it \emph{is} permissible, what do you get?
\end{problem}

\section{Likelihoods}

Imagine you have $N$ data points or measurements of some kind $D_n$,
possbly times or temperatures or brightnesses.  I will say that you
have a ``generative model'' of data point $n$ if you can write down or
calculate a PDF $p(D_n \given \theta, I)$ for the measurement $D_n$,
conditional on a vector or list $\theta$ of parameters and a (possibly
large) number of other things $I$ (``prior information'') on which the
$D_n$ PDF depends, such as assumptions, or approximations, or
knowledge about the noise process, or so on.  If all the data points
are independently drawn (that would be one of the assumptions in $I$),
then the PDF for the full data set $\setofall{D_n}{n=1}{N}$ is just
the product
\begin{eqnarray}\displaystyle\label{eq:likelihood}
p(\setofall{D_n}{n=1}{N} \given \theta, I) &=& \prod_{n=1}^N p(D_n \given \theta, I)
\quad .
\end{eqnarray}
Any kind of PDF for the data is called a ``likelihood'' for historical
reasons I don't care about.  Worse, any kind of conditional PDF for
the data---conditional on parameters---is called a likelihood ``for
the parameters'' even though it is a PDF ``for the data''.\note{I hate
  the terminology ``likelihood'' and ``likelihood for parameters'' but
  again, it is so entrenched, it would be a disservice to the reader
  not to use it.}

Now imagine that the parameters divide into two groups.  One group
$\theta$ are parameters of great interest, and another group $\alpha$
are of no interest.  The $\alpha$ parameters are nuisance parameters.
In this situation, the likelihood can be written
\begin{eqnarray}\displaystyle
p(\setofall{D_n}{n=1}{N} \given \theta, \alpha, I) &=& \prod_{n=1}^N p(D_n \given \theta, \alpha, I)
\quad .
\end{eqnarray}
If you want to make likelihood statements about the important
parameters $\theta$ without committing to anything regarding the
nuisance parameters $\alpha$, you can marginalize rather than infer
them.  You might be tempted to do
\begin{eqnarray}\displaystyle
\mbox{\textbf{wrong:}} & & \int p(D_n \given \theta, \alpha, I)\,\dd\alpha
\quad ,
\end{eqnarray}
but that's not allowed for dimensional arguments given in the previous
\sectionname.  In order to integrate over the nuisances $\alpha$,
something with units of $\alpha^{-1}$ needs to be multiplied in---a
PDF for the $\alpha$ of course:
\begin{eqnarray}\displaystyle\label{eq:mlikelihood}
p(D_n \given \theta, I) &=& \int p(D_n \given \theta, \alpha, I)\,p(\alpha \given \theta, I)\,\dd\alpha
\\
p(\setofall{D_n}{n=1}{N} \given \theta, I) &=& \prod_{n=1}^N p(D_n \given \theta, I)
\quad ,
\end{eqnarray}
where $p(\alpha \given \theta, I)$ is called the ``prior PDF'' for the
$\alpha$ and it \emph{can} depend (but doesn't \emph{need to} depend)
on the other parameters $\theta$ and the more general prior
information $I$.  This marginalization is incredibly useful but note
that it comes at a substantial cost: It required specifying a prior
PDF over parameters that, by assumption, you don't care about!\note{I
  will say a lot more about this in some subsequent \documentname\ in
  this series; in general in real-world problems you need to put a lot
  of care and attention into the parts of the problem that you
  \emph{don't} care about; it is a consequence of the need to make
  precise measurements in the parts of the problem that you \emph{do}
  care about.}  \equationname~(\ref{eq:mlikelihood}) could be called a
``partially marginalized likelihood'' because it is a likelihood (a
PDF for the data) but it is conditional on fewer parameters than the
original, rawest, likelihood.

Sometimes I have heard concern that when you perform the
marginalization (\ref{eq:mlikelihood}), you are allowing the nuisance
parameters to ``have any values they like, whatsoever'' as if you are
somehow not constraining them.  It is true that you are not
\emph{inferring} the nuisance parameters, but you certainly are using
the data to limit their range, in the sense that the integral in
(\ref{eq:mlikelihood}) only gets significant weight where the
likelihood is large.  That is, if the data strongly constrain the
$\alpha$ to a narrow range of good values, then only those good values
are making any (significant) contribution to the marginalization
integral.  That's important!

\section{Posterior probabilities}

A large fraction of the inference that is done in the quantitative
sciences can be left in the form of likelihoods and marginalized
likelihoods, and probably should be.\note{I will say more about what
  you both gain and lose by going beyond the likelihood in a
  subsequent \documentname\ in this series.  It relates to the battles
  between frequentists and Bayesians, which are boring and
  unproductive, in the main.}  However, there are many scientific
questions the answers to which require going beyond the
likelihood---which is a PDF for the data, conditional on the
parameters---to a PDF for the parameters, conditional on the data.

To illustrate, imagine that you want to make a probabilistic
prediction, given your data analysis. For one example, you might want
to predict what $\theta$ values you will find in a subsequent
experiment.  Or, for another, say some parameter (like, say, the age
of the Universe) is a function $t(\theta)$ of the parameters and you
want to predict that parameter (the age) outcome you expect in some
independent measurement of that same parameter (some other experiment
that measures---differently---the age of the Universe).  The
distributions or expectation values for these predictions, conditioned
on your data, will require a PDF for the parameters or functions of
the parameters; that is, if you want the expectation $E(t \given D)$,
where for notational convenience I have defined
\begin{eqnarray}\displaystyle
D &\equiv& \setofall{D_n}{n=1}{N}
\quad ,
\end{eqnarray}
you need to do the integral
\begin{eqnarray}\displaystyle\label{eq:posteriorexpectation}
E(t \given D) &=& \int t(\theta)\,p(\theta \given D, I)\,\dd\theta
\quad ,
\end{eqnarray}
this in turn requires the PDF $p(\theta \given D, I)$ for the
parameters $\theta$ given the data.  This is called the ``posterior
PDF'' because it is the PDF you get \emph{after} digesting the data.

The posterior PDF is obtained by Bayes rule (\ref{eq:bayes})
\begin{eqnarray}\displaystyle\label{eq:posterior}
p(\theta \given D, I) &=& \frac{1}{Z}\,p(D \given \theta, I)\,p(\theta \given I)
\\
Z &\equiv& \int p(D \given \theta, I)\,p(\theta \given I)\,\dd\theta
\quad ,
\end{eqnarray}
where we had to introduce the prior PDF $p(\theta \given I)$ for the
parameters.  The prior can be thought of as the PDF for the parameters
\emph{before} you took the data $D$.  The prior PDF brings in new
assumptions but also new capabilities, because posterior expectation
values as in (\ref{eq:posteriorexpectation}) and other kinds of
probabilistic predictions become possible with its use.

Some notes about all this: \textsl{(a)}~You don't have to use
posterior PDFs exclusively to take expectation values; you can make
predictions that are themselves PDFs; this is in general the only way
to propagate the full uncertainty remaining after your
experiment. \textsl{(b)}~The normalization $Z$ in
\equationname~(\ref{eq:posterior}) is a marginalization of a
likelihood; in fact it is a fully marginalized likelihood, and could
be written $p(D \given I)$.  It has many uses in model evaluation and
model averaging, to be discussed in subsequent \documentnames.
\textsl{(c)}~You can think of the posterior expression
(\ref{eq:posterior}) as being a ``belief updating'' operation, in
which you start with the prior PDF, multiply in the likelihood (which
probably makes it narrower, at least if your data are useful) and
re-normalize to make a posterior PDF.  There is a ``subjective''
attitude to take towards all this that makes the prior and the
posterior PDFs specific to the individual inferrer, while the
likelihood is (at least slightly) more objective.

\section{Advanced topics in inference}

Now imagine that you \emph{really} care about the \emph{prior} on the parameters \ldots hierarchical.

Now come back and imagine you have as many (or more) nuisance parameters than data points!  Is that a problem? \ldots

\section{Example}

Give a simple data analysis problem and show that you have no real
choice---except in what models to consider! \ldots

\clearpage
\markright{Notes}\theendnotes

\clearpage
\begin{thebibliography}{}\markright{References}
\bibitem[Jaynes(2003)]{jaynes}
  Jaynes,~E.~T., 2003,
  \textit{Probability theory:\ The logic of science} (Cambridge University Press)
\bibitem[Mackay(2003)]{mackay}
  Mackay,~D.~J.~C., 2003,
  \textit{Information theory, inference, and learning algorithms} (Cambridge University Press)
\bibitem[Sivia \& Skilling(2006)]{sivia}
  Sivia,~D.~S. \& Skilling,~J., 2006,
  \textit{Data analysis:\ A Bayesian tutorial} (Oxford University Press)
\end{thebibliography}

\end{document}
