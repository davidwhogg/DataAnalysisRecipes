% This document is part of the Data Analysis Recipes project.
% Copyright 2020 the author.

% to-do
% -----
% - make sure I address the illustration of an uncertainty on a function or
%   functional prediction.
% - move problem / exercise macros in from other DARs.
% - make up a toy data set and make problems.
%   - make two different generative models for the toy data.
%   - should have bayes and frequentist options.
% - Cite Numerical Recipes somewhere.
% - Where does the point go that there are many qualitatively different
%   sources of noise or uncertainty.
% - find a place to mention the Neyman--Pearson Lemma (and check spelling!)
% - put some side-note on jackknife about relationship to cross-validation?
% - See notes from Stars & Exoplanets meeting 2020-06-03 for various ideas.
% - Make common style file for both this document and GaussianProductRefactor
% - Before submission: Check that all notes are displayed, and that they are on
%   the correct pages; marginfix is unstable.

\documentclass[10pt]{article}
\usepackage{amsmath, bm, mathrsfs, amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks,
            colorlinks=true,
            linkcolor=NavyBlue,
            citecolor=darkgray,
            urlcolor=NavyBlue]{hyperref}
\usepackage{graphicx}
\usepackage{marginfix} % necessary but possibly evil
% Note to the archaeologists who find this file: The marginfix package is unstable and buggy. It needs to be rewritten.

% citation stuhh
\usepackage{doi}
\usepackage{natbib}
\bibliographystyle{hogg_abbrvnat}
\setcitestyle{round,citesep={,},aysep={}}

% text macros
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}
\newcommand{\notename}{Note}

% math macros
\newcommand{\hquad}{~~}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\tensor}[1]{\mathbf{#1}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
\newcommand{\normal}{\mathcal{N}\!\,}
\newcommand{\like}{\mathscr{L}}

% variables
\newcommand{\va}{\vector{a}}
\newcommand{\vy}{\vector{y}}
\newcommand{\tC}{\tensor{C}}
\newcommand{\tV}{\tensor{V}}
\newcommand{\mX}{\matrix{X}}

% page layout stuhh
\makeatletter
\renewcommand\section{\@startsection {section}{1}{\z@}%
  {-3.5ex \@plus -1ex \@minus -.2ex}%
  {2.3ex \@plus.2ex}%
  {\raggedright\normalfont\Large\bfseries}}
\makeatother
\setlength{\headheight}{2ex}
\setlength{\headsep}{3ex}
\setlength{\parindent}{\baselineskip}
\setlength{\textwidth}{4.3in}
\setlength{\textheight}{2.2\textwidth}
\pagestyle{myheadings}
\markright{\textcolor{gray}{\textsf{Hogg / Estimating uncertainties}}}
\raggedbottom\sloppy\sloppypar\frenchspacing

% this might be crazy, but here's my number
\setlength{\marginparsep}{0.15in}
\setlength{\marginparwidth}{2.7in}
\newcounter{marginnote}
\setcounter{marginnote}{0}
\renewcommand{\footnote}[1]{\refstepcounter{marginnote}\textsuperscript{\themarginnote}\marginpar{\color{darkgray}\raggedright\footnotesize\textsuperscript{\themarginnote}#1}}
\newcommand{\tfigurerule}{\rule{0pt}{1ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{0.25ex}}
\newcommand{\bfigurerule}{\rule{0pt}{0.25ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{1ex}}
\renewcommand{\caption}[1]{\parbox{\marginparwidth}{\footnotesize\refstepcounter{figure}\textbf{\figurename~\thefigure}: {#1}}}

% and make the left margin correct
\setlength{\oddsidemargin}{0.5\paperwidth}
\addtolength{\oddsidemargin}{-1.0in}
\addtolength{\oddsidemargin}{-0.5\textwidth}
\addtolength{\oddsidemargin}{-0.5\marginparwidth}
\addtolength{\oddsidemargin}{-0.5\marginparsep}

% and the top margin
\setlength{\topmargin}{0.5\paperheight}
\addtolength{\topmargin}{-1.0in}
\addtolength{\topmargin}{-0.5\textheight}
\addtolength{\topmargin}{-0.5\headsep}
\addtolength{\topmargin}{-0.5\headheight}

\begin{document}\thispagestyle{plain}

\section*{Data Analysis Recipes: What is \\ the uncertainty on my measurement?}

\textbf{David W. Hogg}\footnote{%
    It is a pleasure to thank
    the participants in my data-analysis classes in New York City,
    and
    the weekly Stars and Exoplanets meeting at the Flatiron Institute
    for help with all of this material, and
    Alessandro Gentilini
    for help with the manuscript.
    If I reach back to my deep past, I find that I am indebted to
    Robert Lupton (Princeton),
    Jonathan Goodman (NYU),
    Gerry Neugebauer (deceased),
    Bill Press (Texas),
    and
    Scott Tremaine (IAS)
    for some of these ideas.} \\
{\footnotesize Center for Cosmology and Particle Physics, Dept Physics, New York University} \\
{\footnotesize Max-Planck-Institut f\"ur Astronomie} \\
{\footnotesize Flatiron Institute}

\paragraph{Abstract:}
Any measurement you make using data ought to be reported with an uncertainty
estimate (often called an ``error'' or an ``error bar'' unfortunately).
I discuss and compare methodologies for making such estimates.
The options available to you depend on whether you have a generative model for your
data, with which either you can simulate your noisy data, or (even better)
you can compute probability densities for different data sets.
They also depend on whether you believe that model,
or believe what it implies for the moments of the noise distribution.
If you have a good generative model, information theory or data simulations can deliver
measurement uncertainties; if you don't they can often provide strong bounds.
Either way, bootstrap and jackknife methods provide well justified, empirical
alternatives that I recommend.
I also discuss the role of nuisance parameters in uncertainty estimation, and
the differences between Bayesian and frequentist approaches and interpretation.
I spend a bit of time on common issues and troubleshooting.
One important idea is that the circumstances in which an uncertainty can be estimated
precisely are rare: Even when you have a very precise measurement, you probably won't
know the uncertainty on that measurement with great precision.
In part for this reason, a (bad) summary of my recommendations is to use
the standard linear or linearized uncertainty, or jackknife, or both.

\section{Measurements and uncertainties}\label{sec:intro}

You have data. There's something you want to measure. You have many
options for making this measurement: You can come up with an
\textsl{estimator}, which transforms your data (through arithmetic
operations) into an estimate of the quantity you want to measure. You
can write down a likelihood function---a probability density function
(or pdf) for your data given the quantity you want to measure (and
maybe other nuisance parameters)---and you can optimize it. That
procedure will get you an estimator, but it would be a
\textsl{maximum-likelihood estimator}, which has some great
properties (to be discussed below).
Or you can write down, in addition to your likelihood
function, a set of prior pdfs over parameters and perform
\textsl{Bayesian inference}.  In each of these cases you will make
some kind of measurement, and in each of these cases you will be
expected to deliver that measurement with an associated \textsl{uncertainty}.

This estimate of your uncertainty can come from different kinds of operations, with
different epistemological status. In some kinds of uncertainty
estimates, we use physical knowledge of the data-generating process to
compute, from (essentially) theory, how the noise in the data
contaminates the measurement. In other kinds of uncertainty estimates,
we use the variance or noise visible in the data to estimate the measurement
uncertainty. Loyal readers of \documentname s in this series can imagine that
I am generally partial to empirical or data-driven uncertainty estimates over
theoretical uncertainty estimates! But I discuss both in detail in what follows,
because (in the physical sciences at least) there are often cases in which the
theoretical uncertainties are very close to correct, or at least very useful.

And---closely related to the above point---the \emph{source} of the
uncertainty can be a well-defined noise process (like photon shot
noise in a detector); or it can be an ill-understood noise process
(like the variabilities of stars, for which we have no good model); or
it can be something else (unknown calibration systematics, mistakes or
wrong approximations in the physical model, and so on). For some analyses
all of these noise processes are relevant; for others only some are.
For example, if you are measuring the mean brightness of a star, the variability
of the star is noise for you, contributing to your uncertainty.
However, if you are measuring the \emph{variability} of the star, the
variability is your signal, not your noise, and it is not a contribution
to your uncertainty at all. But then perhaps the mis-specification of your
variability model will become a new source of uncertainty.
That is, the way uncertainties are estimated has something to do with
\emph{how they will be used} or the objectives of your investigation.
This is related to the (sometimes obscure)
ways that physicists separate uncertainties into ``statistical'' and ``systematic''.
And what's ``systematic'' to some users will be ``statistical'' to
others.\footnote{This is also related to the \emph{subjectivity} of scientific
  investigation. I say a bit more in \notename\textsuperscript{\ref{note:subjective}}.}
We will return to that point below.

One of my grad-school mentors\footnote{This was Gerry Neugebauer (1932--2014).
  He was one of the pioneers of infrared astrophysics, and the US
  lead of the NASA \textsl{IRAS} Mission, and a wonderful human
  being.} liked to say, when I said something about ``errors'' or
``error bars'', that ``they are \emph{uncertainties} not
\emph{errors}! If they were \emph{errors}, we'd correct them!'' That
phrase rings in my head every day. But this points out a difficulty,
which is in the interpretation of uncertainty estimates. When we say
that some parameter is measured to be $42\pm 6$, what does that ``$\pm 6$''
mean?  The answer depends a bit on your statistical philosophy:
If you are a Bayesian, it means that you believe that, with some
fairly well-defined probability (like maybe 68\,percent), the
parameter is within that range.

Seem straightforward? It is, but the straightforwardness of the Bayesian
comes at a cost, which is in assumptions. If you don't want to make some
of those assumptions, you can be a frequentist instead, but then the
interpretation of the measurement $42\pm 6$ is not so simple! For a frequentist
the meaning is that, if the \emph{true}
value of the parameter were $42$,
in some well defined fraction (like maybe 68\,percent)
of hypothetically repeated experiments with similar-quality data the
best-fit or estimated value of the parameter would be within that range.
That is definitely \emph{not} straightforward.
The Bayesian's answer is about the \emph{parameter}, the frequentist's
answer is about hypothetically repeated \emph{counterfactual experiments}.
The trade-off between assumptions made and
simplicity of interpretation will come up again below.
And, combined with this, there are things to say about the terribly named
``confidence interval'' and the even worse-named ``credible region'' and other
abominations.

Most of you, I hope, don't care about such pedantic details and just
want an \emph{uncertainty estimate on your measurement}. In what follows,
I will try to help you get that, with enough discussion to answer questions
and explain your choices. We'll start with the theoretical approaches and
then move on to the empirical approaches, followed by some troubleshooting and
discussion about communication and assessment.

\section{Simplest case: the linear least-square fit}\label{sec:standard}

The simplest case for uncertainty estimation---and a case you may have
encountered previously---is a standard, linear fit, as we have
discussed endlessly in this series.\footnote{See, for example, the
  first two \sectionname s of \cite{straightline}. And also the
  applied math in \cite{gaussianproduct}.}  This is the case in which
the following things hold:
\begin{itemize}
\item You have $N$ data points $y_n$. These could be scalars or vectors, but
  let's call them scalars for now.
\item Each data point $y_n$ has an associated noise variance
  $\sigma^2_n$, which is the variance of zero-mean Gaussian noise that
  affects each data point $y_n$.  You believe these noise variance
  estimates.\footnote{We will return to the question of testing or
    believing your uncertainties below. But one thing to note right here
    is that there are many scientific contexts in which you do not have any
    good idea of these individual-datum uncertainty variances $\sigma^2_n$.
    If you don't know these, you can't use the uncertainty estimate developed in
    this section and you need to use empirical methods like those discussed in
    \sectionname~\ref{sec:empirical}. That is, this assumption---that you have
    some quantitative understanding of the noise in your data---is critical
    to the theoretical or information-theoretic discussion in this \sectionname.}
\item There are no other sources of noise and there is no
  non-Gaussianity (no skewness or kurtosis or outliers or the like) to
  the noise.
\item The noise in data point $y_n$ is independent of the noise
  in any other data point.\footnote{This assumption turns out to be the
    easiest---of all these assumptions---to relax.}
\item There is a set of $K$ features $x_{kn}$ for each data point $y_n$ such that
  the expectation for data point $y_n$ can be \emph{accurately} described
  as a linear combination
  of these features (plus noise).
  That is, the model\footnote{I use the word ``model'' to mean
    a collection of things. It is (at least) a prediction for the data and a
    prediction for the distribution of the noise. A model, for me, is everything
    that you need to construct a likelihood function, or a pdf for the data given
    your parameters.} is appropriate and it can be expressed as
  \begin{equation}
    y_n = \sum_{k=1}^K a_k\,x_{kn} + \mbox{noise}
    \quad,
  \end{equation}
  where the noise on data point $y_n$ is drawn, by assumption, from a distribution
  that is Gaussian, zero mean, and variance $\sigma^2_n$.
\end{itemize}
When \emph{all} of these assumptions hold---or, really, when you are
willing to assume \emph{all} of these things---the maximum-likelihood
estimates\footnote{I will distinguish a parameter,
  for example $a_2$, from an \emph{estimate} of that parameter, for example $\hat{a}_2$.
  The estimate is the outcome of applying an estimator to the data.
  The parameter can in principle take on values other than the estimated value; the
  parameter is a variable whereas the estimate is a value for that variable.}
$\hat{a}_k$ for the parameters $a_k$ can be found by minimizing what
physicists call ``chi-squared''\footnote{For physicists, chi-squared is a statistic
  of the data or an objective function to be minimized. It is like a metric distance
  between the data and the model, where the metric is the inverse variance tensor
  of the data. For statisticians, chi-squared is a distribution. It is the distribution
  you expect for the sum of squares of (zero-mean) Gaussian draws. This difference in
  the meaning of the word ``chi-squared'' causes some
  confusion when we participate in interdisciplinary activities in data analysis.}
\begin{equation}
  \chi^2 = \sum_{n=1}^N \frac{\left[y_n - \sum_k a_k\,x_{kn}\right]^2}{\sigma^2_n}
\end{equation}
The magic\footnote{This magic is described in much more detail in \cite{straightline}.}
is that the maximum-likelihood parameter estimates
$\hat{a}_k$ are given by straightforward linear algebra:\footnote{It is beyond the
  scope of this \documentname, but if you are actually implementing the estimator
  of \equationname~(\ref{eq:llsf}), you don't want to naively construct the
  tensor $\tC$ (which is almost entirely zeros). Write your code to just do the
  diagonal multiply directly. Also, you never want to use the inverse function
  \code{inv()}; instead use the \code{solve()} or \code{lstsq()} function, which
  does the multiply by the inverse more precisely than just naively taking the
  inverse. There are a lot more things to say about implementation that go
  beyond these pieces of advice; I am putting these here just to make the point
  that implementation of the simple formula isn't, itself, necessarily simple.}
\begin{equation}\label{eq:llsf}
  \hat{\va} = [\mX\T\cdot\tC\inv\cdot\mX]\inv\cdot\mX\T\cdot\tC\inv\cdot\vy
\end{equation}
\begin{equation}
  \hat{\va}\T \equiv \begin{bmatrix} \hat{a}_1 & \hat{a}_2 & \hdots & \hat{a}_K \end{bmatrix}
\end{equation}
\begin{equation}
  \mX \equiv \begin{bmatrix}
    x_{11} & x_{21} & \hdots & x_{K1} \\
    x_{12} & x_{22} & \hdots & x_{K2} \\
    x_{13} & x_{23} & \hdots & x_{K3} \\
    \vdots & \vdots &        & \vdots \\
    x_{1N} & x_{2N} & \hdots & x_{KN}
    \end{bmatrix}
\end{equation}
\begin{equation}
  \tC\inv \equiv \begin{bmatrix}
    1/\sigma^2_1 & 0 & 0 & \hdots & 0 \\
    0 & 1/\sigma^2_2 & 0 & \hdots & 0 \\
    0 & 0 & 1/\sigma^2_3 & \hdots & 0 \\
    \vdots & \vdots & \vdots & & \vdots \\
    0 & 0 & 0 & \hdots & 1/\sigma^2_N \\
    \end{bmatrix}
\end{equation}
\begin{equation}
  \vy\T \equiv \begin{bmatrix} y_1 & y_2 & y_3 & \hdots & y_N \end{bmatrix}
  \quad ,
\end{equation}
where $\hat{\va}$ is the (column) vector of parameter estimates $\hat{a}_k$,
$\mX$ is the design matrix of features $x_{kn}$,
$\tC\inv$ is the inverse variance tensor for the data\footnote{Note about
  typesetting: I typeset vectors---which are all column vectors---like
  $\va$, $\vy$. I typeset arbitrary rectangular matrices like $\mX$
  and variance tensors (which must be
  non-negative definite, square matrices) like $\tC$.}
which is diagonal in this simple case),
and
$\vy$ is the (column) vector of data points $y_n$.

When this (\ref{eq:llsf}) is your estimator for $\hat{\va}$, and when
the assumptions (listed above) hold (or you are willing to hold
them\footnote{\label{note:subjective}%
  One of the themes of this \documentname\ and the others in this
  series is that data analyses are always technically \emph{subjective}: You
  make choices, based on your beliefs, and your methods flow from your choices.
  Your estimator and your uncertainty on your estimated values both are
  included in this: You have to make strong assumptions, hopefully based on your
  beliefs, and maybe also on your \emph{utility} (because there are trade offs
  between being accurate with your data and being tractable in your code).
  I don't think this subjectivity is a problem, but I do think that it has to be
  acknowledged and addressed. One of the most important things to note here is
  that although things like Gaussianity and independence are \emph{assumptions},
  these assumptions can be checked, empirically, with statistical tests. That is,
  assumptions are subjective and defining, but they are not outside the realm
  of empirical science. They are part of it.}),
then the formal or standard uncertainty on the $K$-vector of parameter
estimates $\hat{\va}$ is the $K\times K$ uncertainty tensor $\tC_a$
\begin{equation}
  \tC_a = [\mX\T\cdot\tC\inv\cdot\mX]\inv
  \quad.
\end{equation}
How do we interpret this object?

The diagonal elements of the uncertainty tensor $\tC_a$ are the squared uncertainties
(variances of the uncertainty distribution) for the parameters. That is, the
$k,k$ element of $\tC_a$ is the squared uncertainty $\sigma^2_{ak}$ on parameter
$a_k$; in symbols
\begin{equation}
  \sigma^2_{ak} = [\tC_a]_{kk}
  \quad ,
\end{equation}
where by $[\tV]_{ij}$ we mean the $i,j$ element of the matrix $\tV$.
The quantitative meaning of this uncertainty will be addressed below in
\sectionname~\ref{sec:interp}.
These uncertainty estimates are conservative in the sense that the diagonal element of
the tensor $\tC_a$ is the uncertainty on the parameter,
\emph{permitting the other parameters to vary}
or including the uncertainty that comes from the fact that you don't know
those parameters in advance or separately.
This point is related to questions of nuisance parameters (marginalization or
profiling) to be discussed in \sectionname~\ref{sec:nuisances}.
The uncertainty is \emph{not} conservative, however, in the sense that it depends
critically on the assumptions we made at the beginning of this \sectionname,
and especially
that the noise in the data is purely Gaussian with zero mean and with known variances
and uncorrelated.

You might ask: If the diagonal element of the tensor $\tC_a$ is the squared uncertainty
on the parameter, what is the meaning of the diagonal element of the inverse
uncertainty tensor\footnote{An inverse uncertainty tensor
  is sometimes called a \textsl{precision matrix} or \textsl{information tensor},
  for reasons that will appear in \sectionname~\ref{sec:info}.}
$\tC_a\inv$? 
It turns out that the diagonal element of $\tC_a\inv$ is not the inverse of the
square of the uncertainty on the parameter, or it is, but now \emph{not} taking
into account the uncertainties on the other parameters.
That is, the inverse-square-root of the $k,k$ element
of the inverse uncertainty tensor $\tC_a\inv$ is not as
conservative an uncertainty estimate for parameter $a_k$ as is the square-root of the
$k,k$ element of the uncertainty tensor $\tC_a$,\footnote{A figure demonstrating this
  difference would be cool! Hogg}
or in symbols
\begin{equation}
  \sigma^2_{ak} \ne \frac{1}{[\tC_a\inv]_{kk}}
  \quad .
\end{equation}
We will return to this point and discuss it further in
\sectionname s~\ref{sec:interp} and \ref{sec:nuisances}.

Now what about the off-diagonal elements of the uncertainty tensor $\tC_a$?
The quick answer is that if the off-diagonal $k,k'$ element of $\tC_a$ is positive,
then if you (through bad luck) are over-estimating parameter $a_k$ then you are
also likely to be over-estimating parameter $a_{k'}$.
Or, alternatively, then you might know the \emph{difference} between the parameters
(if they have the same units\footnote{Never subtract two quantities that have different
  units---nor add them---for God's sake!}),
or the \emph{ratio} of the parameters more precisely than you know either of them
individually.
On the contrary, if it is negative, then an over-estimate of one parameter should
be associated with an under-estimate of the other, and you might know the sum
or product better than either individually.
In detail it is hard to have good, universal intuitions about these covariances.
I will return to these matters in \sectionname s~\ref{sec:interp} and \ref{sec:viz},
where we look at how to interpret and visualize uncertainties and uncertainty tensors.

At this point we have delivered what is requested: An uncertainty estimate
on the measurement!
If this serves your needs, you are done and you don't need to read further,
unless you want some understanding of what just happened.
Of course for many of you I expect that this does \emph{not} meet your needs:
It might be that your model is nonlinear, so the optimum (the maximum-likelihood
estimator) is not a linear function of the data. That case will be discussed
in \sectionname~\ref{sec:nonlinear}.
It might be that your model is linear but you want to know the uncertainty on
some nonlinear combination of the linear parameters.
If so, see \sectionname~\ref{sec:transform}.
It might be that you don't have an executable or tractable likelihood function.
In this case, you either have to use data simulation to get your uncertainties,
as discussed in \sectionname~\ref{sec:simulation}, or empirical approaches
like bootstrap or jackknife, as discussed in \sectionname~\ref{sec:empirical}.
It might be that I haven't met your needs because you don't have good noise
estimates for your data points, or you aren't using a maximum-likelihood method
at all. Or you don't have a generative model at all. Or you don't believe it, or
want to check it. In these cases, you must use empirical approaches like those discussed
in \sectionname~\ref{sec:empirical}. You have no choice.
And of course I might be missing the point because your analysis is Bayesian.
In that case I will talk to you in \sectionname~\ref{sec:bayes}.

HOGG: EXERCISE HERE where we do repeated experiments generating data
and doing a linear fit and show that the best-fit parameters are
Gaussian-distributed according to the rules above.

\section{Interpretation and discussion of the simple case}\label{sec:interp}

MAKE SURE THAT THIS SECTION ISN'T TOO REPETITIVE with what's above and below.

The standard uncertainty $\sigma_{ak}$ on parameter estimate $\hat{a}_k$
(the square root of the uncertainty variance $\sigma^2_{ak}$)
we found in \sectionname~\ref{sec:standard} is called the ``one-sigma''
uncerainty.
If all the assumptions that led to the analysis were correct---and there
are a lot of assumptions---then the uncertainty variance $\sigma^2_{ak}$
is the expected variance\footnote{Statisticians talk in terms of bias and variance
  of estimators. In usual parlance, these have frequentist meanings, in the sense
  that they refer to the mean and variance in a hypothetical universe in which
  the experiment is repeated, as described in this paragraph.}
of the maximum-likelihood estimator $\hat{a}_k$.
It is the variance in the following sense:
If you repeated the measurement many times, and every time you had similar-quality
data (like similar individual-data-point uncertainty variances $\sigma^2_n$ and
the same number of data points and so on) and always all the assumptions listed
at the beginning of \sectionname~\ref{sec:standard} hold, you would get different
answers\footnote{The idea of this game of hypothetical repeated experiments is
  that the true parameters $\va$ are the same in every repeated experiment, and
  the noise variances $\sigma^2_n$ are the same in every repeated experiment,
  but the specific noise draws or specific data you get is different. So the
  repeated experiments are independent, identically distributed, noisy measurements
  of exactly the same thing, measured in the same way each time.}
every time---different parameter estimates $\hat{a}_k$, but these would
be distributed in a Gaussian distribution with a mean equal to the true values of those
parameters\footnote{Hence, unbiased.}, and variance equal to $\sigma^2_{ak}$.
Or in the multi-dimensional case, the distribution of vectors $\hat{\va}$ would be
Gaussian with mean equal to the true value of $\va$ and variance tensor $\tC_a$,
again as computed in \sectionname~\ref{sec:standard}.
When someone says ``this is a one-sigma uncertainty'', this is usually what
they mean (but not always\footnote{One exception is if the speaker is
  a Bayesian, in which case they might be giving you the root-variance
  or some quantiles of the posterior pdf. We will return to that
  possibility in \sectionname~\ref{sec:bayes}.}).

If you don't have very strong reasons to do otherwise, and you only have one
parameter you deeply care about, and you want to report
a value for your parameter and an uncertainty, report this one-sigma uncertainty!
If you plot your measurement on a plot, plot it as a dot (or other symbol)
plus lines (``error bars'') that extend by one-sigma in each direction.
If a plot contains points and error bars and no detailed description, then this
is how we interpret those error bars: They ought to be one-sigma, standard
uncertainties.

...Now imagine that you have 2 (or worse, $K$) parameters, how do you interpret
the matrix $\tC_a$? Let's start at 2 dimensions. Eigenvalues and eigenvectors make
up an ellipse. Show a figure.

...Does the 68-percent rule change in 2 dimensions, or $K$ dimensions? Argh!!

...In the same way that the uncertainty on one parameter is the $k,k$ element
of $\tC_a$, the uncertainty on a pair is the $2\times 2$ sub-matrix associated
with those two elements.

...Under what circumstances do you want to look at the elements of $\tC_a\inv$?
There are some! They relate to the independent information about each parameter.
Consider, for example, the uncertainty on one parameter,
the uncertainty on the other, and the uncertainty on one \emph{given} the other.

...What would it mean to have no covariance between $k$ and $k'$? The concept
of centering. Is this a correct usage?

...now pivot to discussion of the math of the method...?

Perhaps because I am a physicist, I like to think in terms of units and dimensions.
The linear-algebra objects $\vy$, $\tC$, and $\mX$ have units,
and the result $\hat{\va}$ must also have correct units.
The $N$-vector of data $\vy$ has, say,
``data units'' (maybe photons per second, say, or maybe Kelvins, or maybe
magnitudes or Janskys if you are an astronomer).
Or maybe even each row of $\vy$ has its own special units, if the data are
very heterogeneous.
The inverse variance tensor $\tC\inv$ has inverse-data-squared units, because
it contains inverted, squared uncertainties.
The design matrix $\mX$ has units of ``data per parameter'' because when the
features $x_{kn}$ in the design matrix are multiplied by the parameters $a_k$
in the parameter vector $\va$ they produce predictions for the data.
Again, every row of the parameter vector $\va$ can have different units, but
then every element $x_{kn}$ of the design matrix $\mX$ has units of data element
$y_n$ divided by the units of parameter $a_k$.
Indeed, you can think of $\mX$ as a derivative of the expectation of the data
vector $\vy$ with respect to the parameter vector $\va$
\begin{equation}
  \mX \approx \frac{\dd\vy}{\dd\va}
\end{equation}
where I am being a bit loose with notation because really it is not the
derivative of the data $\vy$ but the derivative of the \emph{expectation} of the
data.
With these observations, it is a nice exercise to show that the estimator
$\hat{\va}$ given in \equationname~(\ref{eq:llsf}) has correct parameter
units, and so does the uncertainty tensor $\tC_a$.\footnote{The
  requirement that equations have correct units---that the left-hand
  and right-hand sides of every equation have the same units---is a
  fundamental symmetry of everything in all of science. Yes, I think
  this is a symmetry.}

And along those lines of units, imagine that you have 2 or more parameters
in your vector $\va$ and you have plotted the 2-or-more-dimensional variance
tensor, using eigenvalues and eigenvectors.
Imagine, further, that the different parameters---the different entries of
the parameter vector $\va$---have different units.
What are the meanings of the eigenvalues and eigenvectors of the variance
tensor $\tC_a$?
....HOGG

One important (and perhaps upsetting) point:
The parameter uncertainty variance estimate
$\tC_a = [\mX\T\cdot\tC\inv\cdot\mX]\inv$
is based on the design matrix $\mX$ and the noise variance for the data $\tC$,
but it makes no reference to the data $\vy$ themselves!
That is, the data don't enter, only the precision of the data enters.
That is, the uncertainty variance could be computed on the basis of the
expected variance without any idea about the expected data.
Or even the parameter values.
See how strange this is?
This is one of the indicators that the standard uncertainty estimate is a
very theoretical object, and very brittle. We'll say more about this below.
But first let's see how this uncertainty estimate generalizes and is justified.

HOGG: EXERCISE HERE repeat problem above but now draw the ellipse and show
that about k-percent of them are inside the ellipse.

\section{A common fallacy about chi-squared}\label{sec:fallacy}

The uncertainty can be thought of as the distance you are permitted to move
in the parameter space (the space in which the vector $\va$ lives)
without changing chi-squared by more than about 1.
That is, if you start at the minimum-chi-squared or best-fit parameter
vector position $\hat{\va}$ and you move one of the parameters $a_k$ by some small
amount $\delta a_k$, the displacement $\delta a_k$ is within one standard error
if the chi-squared
value did not rise from its minimum by more than $\Delta\chi^2=1$.\footnote{Demonstrating
  this, either analytically or numerically, is a nice exercise for the ambitious reader.
  HOGG: MOVE THIS COMMENT UP TO THE PREVIOUS SECTION.}
Another way to put this is as follows:
The square-root-eigenvalues and eigenvectors of $\tC_a$ are the orthogonal
semi-axes of an ellipsoid in parameter space describing the parameter-space
volume in which you are permitted to move the parameters away from the
minimum-chi-squared solution without increasing chi-squared
away from its minimum value by more than 1. We will come back to the computation
and visualization of this ellipsoid in \sectionname~\ref{sec:viz}.

But this description sometimes makes physicists uneasy. After all,
we are also taught (often) that if the model we are fitting is a \emph{good fit},
then chi-squared is expected to be $\nu\pm\sqrt{2\,\nu}$ HOGG CHECK
where $\nu$ is the number
of degrees of freedom or the number of data points minus the number of parameters.
That is, chi-squared is supposed to have a value that is related to the number
of data points, and if it is far away from that value, there is something wrong
with either your model or your uncertainty estimates.
That is, chi-squared is often used as a test of whether the model is \emph{a good fit}.
Now imagine that $\nu$ is large (as it often is). That is, imagine that you have
many data points.
In this case, the model-acceptance $\chi^2$ volume (which goes out to
$\Delta\chi^2$ of something like $\sqrt{2\,\nu}$) in parameter space
is \emph{far larger} than the uncertainty volume (which goes out to
$\Delta\chi^2$ of unity).
What gives? Which is my uncertainty?

The answer is that your uncertainty is what we delivered in
\sectionname~\ref{sec:standard}.
This relates, somehow, to the difference between \emph{precision} and \emph{accuracy}:
The data can be very precise, whether or not the model is accurate.
And determining the value of a parameter \emph{within the context of a
  model} is much easier and more precise than determining
\emph{whether that model is appropriate}.
These two things have been famously confused in the
literature.\footnote{CITE Gould maybe? I often surprise even seasoned
  data analysts with these observations. Confused? It's genuinely
  confusing.}

The important thing is that the uncertainty can be small whether or
not the model is ruled out in the ``bad chi-squared'' sense. Indeed,
the uncertainty estimate of \sectionname~\ref{sec:standard} makes no
reference to the data $\vy$ at all!
And certainly no reference to the value of chi-squared at the best-fit
model.
These issues (parameter uncertainty \foreign{vs} goodness of fit) are
totally different.
The uncertainty is about differences in likelihood within the context
of a model that is \emph{literally assumed to be correct} in some sense.
Recall the assumptions listed in \sectionname~\ref{sec:standard}!
The model testing (is chi-squared of order $\nu$) is about comparing
the data to a theoretical expectation of the model. That's different.
It does \emph{test} the assumption that the model is applicable. But the
parameter estimation can happen and be precise in either case.

% HOGG: SHOULD IT BE chi-square OR chi-squared??

When I speak about these matters, I like to say ``chi-squared is a measure
of the size of your data'', which I learned from some statistician.\footnote{I am
  embarrassed to say that I don't remember which one.}
This adage is true for two reasons.
The first is that chi-squared rises with the number of data points, because
(even when all is well) it has a scaling with the number of degrees of freedom
$\nu$, which itself is the number of data points minus the number of
parameters.
The second sense in which this adage is true is that as you get more data it
becomes more likely that you will rule out your model.
A model that passes the chi-squared test for a small or imprecise data set
will fail that same test for a large or precise data set.
And since all models are wrong (in detail), all models will fail the chi-squared
test if you have the patience to take enough data.
Before we go back to our main subject---uncertainty estimation---I want to just
say for the last time: The standard uncertainty estimate does not depend on
whether the adopted model is a good fit or strongly ruled out! It doesn't depend
on the absolute value of chi-squared at all.

\section{Generalization to nonlinear models}\label{sec:nonlinear}

If you want to derive the magical linear least-squares estimator (\ref{eq:llsf}) above,
or if you want to generalize to nonlinear models,
it is useful to note that the linear-algebra objects defined
above have the following properties:
The objective function $\chi^2$ can be written as
\begin{equation}
  \chi^2(\va) = [\vy - \mX\cdot\va]\T\cdot\tC\inv\cdot[\vy - \mX\cdot\va]
  \quad,
\end{equation}
where we have written this as a function of the parameter vector $\va$ because
that is the thing with respect to which we minimized to get the estimates
$\hat{\va}$.
The likelihood function---the probability density function (pdf) for
the data given the parameters---can be written as
\begin{equation}
  p(\vy\given\va) = \normal(\vy\given\mX\cdot\va,\tC)
  \quad,
\end{equation}
where $\normal(x\given m,V)$ is the Gaussian pdf given mean $m$ and variance $V$.
The log-likelihood function is
\begin{equation}
  \ln\like(\va) = -\frac{1}{2}\,\chi^2(\va) -\frac{1}{2}\,\ln\det(2\pi\,\tC)
  \quad,
\end{equation}
where $||V||$ is the determinant operator and we have written the
log-likelihood as a function of the parameters $\va$ even though it is
also a function of the data $\vy$ and the noise variance $\tC$.
We write it as a function only of the parameters because when we manipulate it
(in a moment), we only take derivatives with respect to these parameters.

If we take one derivative of $\ln\like(\va)$ with respect to $\va$ and set it
to zero, we find the optimum given by the magic formula in
\equationname~(\ref{eq:llsf}).
If we take two derivatives of $\ln\like(\va)$ with respect to $\va$ we get
\begin{equation}
  -\frac{\dd^2}{\dd\va^2}\,\ln\like(\va) = \mX\T\cdot\tC\inv\cdot\mX = \tC_a\inv
  \quad.
\end{equation}
That is, the (negative of the) second derivative of the log-likelihood function is
the inverse variance tensor $\tC_a\inv$.
That's important for generalizing the simple, linear case.

...Comment on units and shapes of objects.

...Non-linear fit. What's different?

...The concept of linearizing and then doing what you did above. Note that this
requires a choice of a fiducial point around which to linearize.

...A reminder that uncertainties are rarely precisely known, so often linearization
is as good as you need; do it and publish and stop reading now.

If you want more theoretical backing, read \sectionname~\ref{sec:info}. If you
don't trust your likelihood function, head to \sectionname~\ref{sec:empirical}.
If you not only believe your likelihood function, but you are happy to put priors
on all your parameters---or if you want to run MCMC---check
out \sectionname~\ref{sec:bayes}.

\section{Information theory}\label{sec:info}

...NOTE language that the inverse variance is called the information tensor or the precision matrix or other such things.

...Connection to Cram\'er--Rao bound and Fisher information.

Relationship of first derivative squared to second derivative.

Note inherent frequentism in this point of view.

...Come back to inverse-variance weighting. Note that this appears everywhere
in the linear case. That's deep information-theory shih.

\section{Bayes}\label{sec:bayes}

If your data analysis is Bayesian,\footnote{There are a lot of things that
  can be meant by the word ``Bayesian'' but for our purposes here it only means
  that you are doing your data analysis with a likelihood function \emph{and} a
  prior pdf over the parameters. The information-theoretic considerations in the
  previous \sectionname s are explicitly frequentist in justification, but (as so
  often happens in data analysis) the final results are very similar between
  frequentist and Bayesian uncertainty estimates. It is mainly their
  interpretations that differ. Not all statisticians would agree with me
  on these points.}
and you are obeying the rules of
Bayesian inference, then the uncertainty estimation and noise propagation
are automatically built in to your analysis.
That is, in Bayesian inference, you are required to have a likelihood
function\footnote{Of course there are Bayesian likelihood-free methods
  such as (the terribly named) Approximate Bayesian Computation (see, for
  example, \cite{wiki:abc}). These methods are data-simulation methods and
  live somewhere between the things I'm writing about in this \sectionname\ and
  the things in \sectionname~\ref{sec:simulation}.} and also a prior, and your
parameter estimation or measurement is performed by multiplying these
two and displaying or summarizing the posterior pdf for the parameters
of interest.
Provided that you believe your likelihood function (which, if you are
a subjective Bayesian, you explicitly must) and you believe your prior
pdf (same), then you don't have to do anything else at all: Your posterior
pdf is literally a probability density over all possible parameters that
are consistent with your prior and your data. It contains all of the
measurement and uncertainty that you could possibly want or need to know.
You have no choice, and you get a full answer.

...All that said ... So you better believe that model really really well...
Or make it more baroque. (Good option for a problem / exercise!)

...Now consider the fully Gaussian linear model again and note that the
Bayesian uncertainty has closed form and is incredibly similar to the frequentist
result; the only difference is that the prior pdf appears like an additional
data point. And there is an additional---critical---assumption.

...Now consider the linearized non-linear case. Same thing: very similar to
the frequentist result.

...In general...it's a mess but you get to use MCMC (note and cite). MCMC is a
method of last resort but it requires little thinking (note on climate impact).

...Note that you can't run MCMC without priors. And no, flat priors are not no
priors. A sampling \emph{has no meaning} without priors. Why not?

...And the theorem that Goodman pointed me to that give some limits in which
Bayesian and frequentist confidence intervals are identical.

...Now am I bringing together frequentists and Bayesians? No, because the
interpretation is \emph{totally different}. In the one you have figured out the
probability in which your experiment would get your data, given a particular
parameter value; while in the other you have figured out the probability that
the universe has your parameter value, given your data. So be very careful
what you say in your papers, people. Take some time to criticize but explain the
confidence vs credible terminology.

...But I will say something that both frequentists and Bayesians can agree on:
Lots of investigators vary the priors, but it is the likelihood function
that needs to be challenged, usually! Cite Gelman.

\section{Data simulation}\label{sec:simulation}

Everything in the preceding \sectionname s has assumed that you are in possession
of an executable likelihood function, or probability (or really pdf)
for your data given the parameters.
In many cases you don't have such a function, either because you don't have
a well-formed belief about how your data are generated (or the noise process
in particular), or because you do, but you can't tractably compute the
pdf for the data evaluated at the parameters.
This situation is not uncommon; we encounter it, for example,
when we measure large-scale structure (two-point and higher-order statistics)
in cosmology.
When you don't have a tractable likelihood function, you have two-ish choices.
One is to perform data simulations, which is the subject of this \sectionname,
and the other is to use empirical measures of the uncertainty like bootstrap and
jackknife, which are the subject of \sectionname~\ref{sec:empirical}.

As we said in \sectionname~\ref{sec:intro}, the
frequentist definition of uncertainty is literally tied to what you
\emph{would have gotten} if you had repeated the experiment.
And that is also the implicit setting for the information-theoretic
derivation and discussion in \sectionname~\ref{sec:info}.
So---it stands to reason that---if you can simulate your data with a
simulation you believe, you can generate standard uncertainties
directly through simulation.
The idea is simple: Make a simulation of the data at known true\footnote{The
  word ``true'' is problematic, and my loyal reader knows that I avoid using
  this word. Or use it only under strict conditions. Here the word ``true'' is
  only used to mean the parameters that you---as the investigator---set for
  your data simulation. When you simulate data you are acting like God, so you
  can set parameters like God, and God's parameters are the true parameters.
  In the Real World (tm), you can never know the true parameters. They might not
  even exist (because your model is wrong or approximate in important ways).}
parameters $\tilde{\va}$, obtain estimated values for the parameters
$\hat{\va}$ in that simulated data set using your estimator, repeat
$K$ times, and then estimate the mean and variance (or full distribution) of
the mistakes you made.

...why am I talking about ``mistakes''? Well, when you know the truth and you have
an estimate, there is a difference between them. This is not the uncertainty; it
is a draw from the distribution over uncertainties. And I am not going to call
it an ``error'' because that term is over-loaded (see \sectionname~\ref{sec:intro}).

...note that you have to choose a fiducial point in parameter space.

...note that you can compute bias also, which is interesting. And the whole
distribution of mistakes.

...How many simulations do you need?

...Different kinds of parameters and resimulation?

...Emulators?

...What do you want to compute, really? Come back to the mistakes of LSS.

\section{Jackknife and bootstrap}\label{sec:empirical}

In many---or even most---of the data analysis projects in which I have been
involved, we have \emph{not} confidently believed all of our assumptions about the
noise processes contributing to our data.
That is, we have not been confident that we have a realistic model of how
the data were generated.
This means that even if we can compute a likelihood function, we don't entirely
believe it.
And even if we can simulate our data, we don't believe that the simulated data
will have all the issues that beset our real data.
We cry out for an empirical method to assess our uncertainties!

The statistics community hears our cries, and has responded with many options.
Here we will discuss two that can be used (with care, perhaps) in almost every
setting I regularly encounter: Jackknife and bootstrap.

...Remind the reader that we didn't use the agreement between model and data
at all in the above!!

...Remind the reader that if our data is very noisy, the scatter around the
model prediction or expectation will be large. So the noise variance is empirically
accessible.

...etc.

...Extremely important point: You must bootstrap UP FRONT. Not after
data pre-processing or patching or whatever. It has to happen before
you do anything to the data. Before the train/test/validation split. Etc.
Call out the APW project here.

...At the end come back to the point that even if you DO believe your noise
model and your LF you still want to do jackknife to check it all. This test
MUST PASS.

\section{Nuisance parameters}\label{sec:nuisances}

As we have implicitly discussed above, in most substantial data analyses, your
model includes both important parameters you care about, like, say, the age of
the Universe, and unimportant nuisance parameters you don't, like, say, the
conversion between counts in your detector and the brightnesses of the supernovae
you are measuring.
If you are being conservative, you want to provide the final measurement uncertainty
on the age of the Universe, including the effective noise contribution or uncertainty
contribution from the unknown (and uninteresting) calibration parameters.\footnote{I
  might comment here that I have spent a lot of my life determining calibration
  parameters and I have even recommended that we---in some cases---think of the
  calibration parameters as the important parameters, and the properties of the
  Universe as the nuisances (\citealt{hoggcrazy}).
  So take this to be an example only; one scientist's
  nuisance is another's objective.}
How you account for these effects---how you propagate the uncertainty from the
nuisance estimation---depends on how you are getting your uncertainties in the
first place.
It isn't difficult, but it requires some care and attention in some cases.

Before I continue, I should remark that the inclusion or exclusion of
nuisance parameters in fits can be a controversial subject, and is a
place that investigators can introduce important but hard-to-catch
adjustments to their uncertainty estimates.
That is, two investigators, making the same measurement with the same data
with the same likelihood function can get very different uncertainty estimates
simply by including or excluding certain nuisance parameters in their
fits.\footnote{This is not science fiction: Different cosmological
  projects in my lifetime have obtained very different uncertainty
  estimates by permitting different parameters to vary; funding for a
  project can be granted or declined on the basis of the forecast
  precision of that project; the temptation to fix important
  parameters can become overwhelming. Some of these issues are discussed
  implicitly by \cite{fom}. That reference also contains a very compact
  and useful discussion of the relevant information theory around nuisances.}

Why is it that the introduction of a free nuisance parameter will in general
increase the uncertainty on your parameters of interest? Hogg....

... HOGG: Difference between marginalizing and profiling.

Identicality when it comes to the linear, Gaussian case.

What it looks like.

What it looks like in very nonlinear situations (like period fitting).

\section{Transformations of parameters}\label{sec:transform}

...You have an error estimate on some parameters, but you want to know the
uncertainty on some function of those parameters. Examples...

HOGG: Imagine that you have $\hat{\va}$ and you have $\tC_a$. What is the
uncertainty on $f(\va)$? Is it just the same as doing inference, where
the $\hat{\va}$ is your data vector? I think it might be.

\section{Systematic error and theoretical uncertainty}

There is only bias and variance; nothing else.

What is a statistical uncertainty? And what is a systematic one?

When do you want to use the former? When do you want to add both in quadrature?

\section{Visualizing and reporting uncertainties}\label{sec:viz}

error bars

68, 95, 99.whatever and all that.

mean, variance, skewness, kurtosis and all that.

asymmetric erro bars.

violin plots.

banana plots.

corner plots.

catalogs and data releases. Cite Malz.

\section{Other unpleasant situations}

Upper limits and how to use them. And why NOT to report them. What are
you trying to report?

Related: What if your LF is undefined for, say, a negative eccentricity?

Related: What if your LF is defined for a negative parameter, but your
prior rules it out? Like the brightness of a star?

What if your uncertainties are part of the model? This is possibly
true if you are using a GP for your noise, say. Or if your
uncertainties depend on your model parameters, as they do in a Poisson
process. Nothing really changes, but say words.

\section{Common mistakes and troubleshooting}

My measurement is ridiculously precise! This makes no sense!

About 68\,percent of your values should be outside one sigma of the
mean prediction (expectation) of your model. What do think or do if
that's not true?

The uncertainty on the mean vs the distribution of values.

My data aren't well fit by my model; what does that mean for my uncertainty
analysis?

Do I multiply my uncertainties up or do I add something in quadrature?

Do I multiply my uncertainties down?

Sometimes you only need the error bars to be correct in a
\emph{relative} sense. If, say they are just being used to weight (by
their squared inverses) data in a fit.

Sometimes you have huge theoretical uncertainties and you want to use
them in your analysis. Sometimes you have these and you \emph{don't} want
to use them in your analysis.

I have a good LF but then I did jackknife and it disagrees!

\section{Discussion}

Hello world.

% Render the references
\clearpage\raggedright
\bibliography{refs}

\end{document}
