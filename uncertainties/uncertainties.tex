% This document is part of the Data Analysis Recipes project.
% Copyright 2020 the author.

% to-do
% -----
% - make up a toy data set and make problems.
%   - make two different generative models for the toy data.
%   - should have bayes and frequentist options.
% - reformat in side-notes style like GaussianProductRefactor.
% - get BibTeX working like GPR.
% - Where does the point go that there are many qualitatively different
%   sources of noise or uncertainty.
% - See notes from Stars & Exoplanets meeting 2020-06-03.

\documentclass[12pt, letterpaper]{article}

\newcommand{\documentname}{\textsl{Note}}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\section*{Data Analysis Recipes:\\
  What is the uncertainty on my measurement?}

\textbf{David W. Hogg} \\
{\footnotesize New York University} \\
{\footnotesize Max-Planck-Institut f\"ur Astronomie} \\
{\footnotesize Flatiron Institute}

\paragraph{Abstract:}
Any measurement you make using data ought to be reported with an uncertainty
estimate (often called an ``error'' or an ``error bar'' unfortunately).
I discuss and compare methodologies for making such estimates.
The options availble to you depend on whether you have a generative model for your
data, with which either you can simulate your noisy data, or (even better)
compute probability densities for different data sets.
They also depend on whether you believe that model,
or believe what it implies for the moments of the noise distribution.
If you have a good generative model, information theory or data simulations can deliver
measurement uncertainties; if you don't they can often provide strong bounds.
Either way, bootstrap and jackknife methods provide well justified, empirical
alternatives that I recommend.
I also discuss the role of nuisance parameters in uncertainty estimation, and
the differences between Bayesian and frequentist approaches and interpretation.
I spend a bit of time on common issues and troubleshooting.
One important idea is that the circumstances in which an uncertainty can be estimated
precisely are rare: Even when you have a very precise measurement, you probably won't
know the uncertainty on that measurement with great precision.

\section{Measurements and uncertainties}

You have data. There's something you want to measure. You have many
options for making this measurement: You can come up with an
\textsl{estimator}, that transforms your data (through arithmetic
operations) into an estimate of the quantity you want to measure. You
can write down a likelihood function---a probability density function
(or pdf) for your data given the quantity you want to measure (and
maybe other nuisance parameters)---and you can optimize it. That
procedure will get you an estimator, but it would be a
\textsl{maximum-likelihood estimator}, which has some great
properties (to be discussed below).
Or you can write down, in addition to your likelihood
function, a set of prior pdfs over parameters and perform
\textsl{Bayesian inference}.  In each of these cases you will make
some kind of measurement, and in each of these cases you will be
expected to deliver that measurement with an associated \textsl{uncertainty}.

This estimate of your uncertainty can come from different kinds of operations, with
different epistemological status. In some kinds of uncertainty
estimates, we use physical knowledge of the data-generating process to
compute, from (essentially) theory, how the noise in the data
contaminates the measurement. In other kinds of uncertainty estimates,
we use the variance or noise visible in the data to estimate the measurement
uncertainty. Loyal readers of \documentname s in this series can imagine that
I am generally partial to empirical or data-driven uncertainty estimates over
theoretical uncertainty estimates! But I discuss both in detail in what follows,
because (in the physical sciences at least) there are often cases in which the
theoretical uncertainties are very close to correct, or at least very useful.

And---closely related to the above point---the \emph{source} of the
uncertainty can be a well-defined noise process (like photon shot
noise in a detector); or it can be an ill-understood noise process
(like the variabilities of stars, for which we have no good model); or
it can be something else (unknown calibration systematics, mistakes or
wrong approximations in the physical model, and so on). For some analysses
all of these noise processes are relevant; for others only some are.
That is, the way uncertainties are estimated has something to do with
\emph{how they will be used}. This is related to the (sometimes obscure)
ways that physicists separate uncertainties into ``statistical'' and ``systematic''.
And what's ``systematic'' to some users will be ``statistical'' to others.
We will return to that point below.

One of my grad-school mentors\footnote{Gerry Neugebauer (1932--2014),
  who was one of the pioneers of infrared astrophysics, and the US
  lead of the NASA \textsl{IRAS} Mission, and a wonderful human
  being.} liked to say, when I said something about ``errors'' or
``error bars'', that ``they are \emph{uncertainties} not
\emph{errors}! If they were \emph{errors}, we'd correct them!'' That
phrase rings in my head every day. But this points out a difficulty,
which is in the interpretation of uncertainty estimates. When we say
that some parameter is measured to be $42\pm 6$, what does that ``$\pm 6$''
mean?  The answer depends a bit on your statistical philosophy:
If you are a Bayesian, it means that you believe that, with some
fairly well-defined probability (like maybe 68\,percent), the
parameter is within that range.

Seem straightforward? It is, but the straightforwardness of the Bayesian
comes at a cost, which is in assumptions. If you don't want to make some
of those assumptions, you can be a frequentist instead, but then the
interpretation of the measurement $42\pm 6$ is not so simple! For a frequentist
the meaning is that, if the \emph{true}
value of the parameter were $42$,
in some well defined fraction (like maybe 68\,percent)
of hypothetically repeated experiments with similar-quality data the
best-fit or estimated value of the parameter would be within that range.
That is definitely \emph{not} straightforward.
The Bayesian's answer is about the \emph{parameter}, the frequentist's
answer is about hypothetically repeated \emph{counterfactual experiments}.
The trade-off between assumptions made and
simplicity of interpetation will come up again below.
And, combined with this, there are things to say about the terribly named
``confidence interval'' and the even worse-named ``credible region'' and other
abominations.

Most of you, I hope, don't care about such pedantic details and just
want an \emph{uncertainty estimate on your measurement}. In what follows,
I will try to give you that, with enough discussion to answer questions
and explain your choices. We'll start with the theoretical approaches and
then pivot to the empirical approaches.

\section{The standard errors on a least-square fit}

Hello world. Linear fit.

Non-linear fit.

\section{Information theory}

Connection to Cram\'er--Rao bound and Fisher information.

\section{Bayes}

Foo

You are only permitted to make one kind of uncertainty estimate in Bayes.

So you better believe that model really really well.

Or make it more baroque. (Good option for a problem / exercise!)

\section{Data simulation}

Hello world.

\section{Jackknife and bootstrap}

Hello world.

\section{Nuisance parameters}

Difference between marginalizing and profiling.

Identicality when it comes to the linear, Gaussian case.

What it looks like.

What it looks like in very nonlinear situations (like period fitting).

\section{Systematic error and theoretical uncertainty}

There is only bias and variance; nothing else.

What is a statistical uncertainty? And what is a systematic one?

When do you want to use the former? When do you want to add both in quadrature?

\section{Common mistakes and troubleshooting}

About 68\,percent of your values should be outside one sigma. What do think
or do if that's not true?

The uncertainty on the mean vs the distribution of values.

My data aren't well fit by my model; what does that mean for my uncertainty
analysis?

Do I multiply my uncertainties up or do I add something in quadrature?

Do I multiply my uncertainties down?

Sometimes you only need the error bars to be correct in a
\emph{relative} sense. If, say they are just being used to weight (by
their squared inverses) data in a fit.

Sometimes you have huge theoretical uncertainties and you want to use
them in your analysis. Sometimes you have these and you \emph{don't} want
to use them in your analysis.

\section{Discussion}

Hello world.

\end{document}
