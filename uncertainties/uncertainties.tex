% This document is part of the Data Analysis Recipes project.
% Copyright 2020 the author.

% to-do
% -----
% - move problem / exercise macros in from other DARs.
% - make up a toy data set and make problems.
%   - make two different generative models for the toy data.
%   - should have bayes and frequentist options.
% - get BibTeX working like GPR.
% - Where does the point go that there are many qualitatively different
%   sources of noise or uncertainty.
% - find a place to mention the Neyman--Pearson Lemma (and check spelling!)
% - put some side-note on jackknife about relationship to cross-validation?
% - See notes from Stars & Exoplanets meeting 2020-06-03 for various ideas.
% - Make common style file for both this document and GaussianProductRefactor
% - Before submission: Check that all notes are displayed, and that they are on
%   the correct pages; marginfix is unstable.

\documentclass[10pt]{article}
\usepackage{amsmath, bm, mathrsfs, amssymb}
\usepackage[dvipsnames]{xcolor}
\usepackage[hidelinks,
            colorlinks=true,
            linkcolor=NavyBlue,
            citecolor=darkgray,
            urlcolor=NavyBlue]{hyperref}
\usepackage{graphicx}
\usepackage{marginfix} % necessary but possibly evil
% Note to the archaeologists who find this file: The marginfix package is unstable and buggy. It needs to be rewritten.

% citation stuhh
\usepackage{doi}
\usepackage{natbib}
\bibliographystyle{hogg_abbrvnat}
\setcitestyle{round,citesep={,},aysep={}}

% text macros
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\documentname}{\textsl{Note}}
\newcommand{\sectionname}{Section}
\newcommand{\equationname}{equation}

% math macros
\newcommand{\hquad}{~~}
\newcommand{\given}{\,|\,}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\!\mathsf{T}\!}}
\newcommand{\inv}{^{-1}}
\newcommand{\scalar}[1]{#1}
\renewcommand{\vector}[1]{\boldsymbol{#1}}
\newcommand{\tensor}[1]{\mathbf{#1}}
\renewcommand{\matrix}[1]{\mathsf{#1}}
\newcommand{\normal}{\mathcal{N}\!\,}
\newcommand{\like}{\mathscr{L}}

% variables
\newcommand{\va}{\vector{a}}
\newcommand{\vy}{\vector{y}}
\newcommand{\tC}{\tensor{C}}
\newcommand{\mX}{\matrix{X}}

% page layout stuhh
\setlength{\headheight}{0.0in}
\setlength{\headsep}{0.0in}
\setlength{\parindent}{\baselineskip}
\setlength{\textwidth}{4.3in}
\setlength{\textheight}{2\textwidth}
\raggedbottom\sloppy\sloppypar\frenchspacing

% this might be crazy, but here's my number
\setlength{\marginparsep}{0.15in}
\setlength{\marginparwidth}{2.7in}
\newcounter{marginnote}
\setcounter{marginnote}{0}
\renewcommand{\footnote}[1]{\refstepcounter{marginnote}\textsuperscript{\themarginnote}\marginpar{\color{darkgray}\raggedright\footnotesize\textsuperscript{\themarginnote}#1}}
\newcommand{\tfigurerule}{\rule{0pt}{1ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{0.25ex}}
\newcommand{\bfigurerule}{\rule{0pt}{0.25ex}\\ \rule{\marginparwidth}{0.5pt}\\ \rule{0pt}{1ex}}
\renewcommand{\caption}[1]{\parbox{\marginparwidth}{\footnotesize\refstepcounter{figure}\textbf{\figurename~\thefigure}: {#1}}}

% and make the left margin correct
\setlength{\oddsidemargin}{0.5\paperwidth}
\addtolength{\oddsidemargin}{-1.0in}
\addtolength{\oddsidemargin}{-0.5\textwidth}
\addtolength{\oddsidemargin}{-0.5\marginparwidth}
\addtolength{\oddsidemargin}{-0.5\marginparsep}

% and the top margin
\setlength{\topmargin}{0.5\paperheight}
\addtolength{\topmargin}{-1.0in}
\addtolength{\topmargin}{-0.5\textheight}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\section*{Data Analysis Recipes: What is \\ the uncertainty on my measurement?}

\textbf{David W. Hogg}\footnote{%
    It is a pleasure to thank
    the participants in my data-analysis classes in New York City,
    and
    the weekly Stars and Exoplanets meeting at the Flatiron Insititute
    for help with all of this material, and
    Alessandro Gentilini
    for help with the manuscript.
    If I reach back to my deep past, I find that I am indebted to
    Robert Lupton (Princeton),
    Jonathan Goodman (NYU),
    and
    Gerry Neugebauer (deceased)
    for some of these ideas.} \\
{\footnotesize Center for Cosmology and Particle Physics, Dept Physics, New York University} \\
{\footnotesize Max-Planck-Institut f\"ur Astronomie} \\
{\footnotesize Flatiron Institute}

\paragraph{Abstract:}
Any measurement you make using data ought to be reported with an uncertainty
estimate (often called an ``error'' or an ``error bar'' unfortunately).
I discuss and compare methodologies for making such estimates.
The options available to you depend on whether you have a generative model for your
data, with which either you can simulate your noisy data, or (even better)
you can compute probability densities for different data sets.
They also depend on whether you believe that model,
or believe what it implies for the moments of the noise distribution.
If you have a good generative model, information theory or data simulations can deliver
measurement uncertainties; if you don't they can often provide strong bounds.
Either way, bootstrap and jackknife methods provide well justified, empirical
alternatives that I recommend.
I also discuss the role of nuisance parameters in uncertainty estimation, and
the differences between Bayesian and frequentist approaches and interpretation.
I spend a bit of time on common issues and troubleshooting.
One important idea is that the circumstances in which an uncertainty can be estimated
precisely are rare: Even when you have a very precise measurement, you probably won't
know the uncertainty on that measurement with great precision.
In part for this reason, a (bad) summary of my recommendations is to use
the standard linear or linearized uncertainty, or jackknife, or both.

\section{Measurements and uncertainties}

You have data. There's something you want to measure. You have many
options for making this measurement: You can come up with an
\textsl{estimator}, that transforms your data (through arithmetic
operations) into an estimate of the quantity you want to measure. You
can write down a likelihood function---a probability density function
(or pdf) for your data given the quantity you want to measure (and
maybe other nuisance parameters)---and you can optimize it. That
procedure will get you an estimator, but it would be a
\textsl{maximum-likelihood estimator}, which has some great
properties (to be discussed below).
Or you can write down, in addition to your likelihood
function, a set of prior pdfs over parameters and perform
\textsl{Bayesian inference}.  In each of these cases you will make
some kind of measurement, and in each of these cases you will be
expected to deliver that measurement with an associated \textsl{uncertainty}.

This estimate of your uncertainty can come from different kinds of operations, with
different epistemological status. In some kinds of uncertainty
estimates, we use physical knowledge of the data-generating process to
compute, from (essentially) theory, how the noise in the data
contaminates the measurement. In other kinds of uncertainty estimates,
we use the variance or noise visible in the data to estimate the measurement
uncertainty. Loyal readers of \documentname s in this series can imagine that
I am generally partial to empirical or data-driven uncertainty estimates over
theoretical uncertainty estimates! But I discuss both in detail in what follows,
because (in the physical sciences at least) there are often cases in which the
theoretical uncertainties are very close to correct, or at least very useful.

And---closely related to the above point---the \emph{source} of the
uncertainty can be a well-defined noise process (like photon shot
noise in a detector); or it can be an ill-understood noise process
(like the variabilities of stars, for which we have no good model); or
it can be something else (unknown calibration systematics, mistakes or
wrong approximations in the physical model, and so on). For some analyses
all of these noise processes are relevant; for others only some are.
That is, the way uncertainties are estimated has something to do with
\emph{how they will be used}. This is related to the (sometimes obscure)
ways that physicists separate uncertainties into ``statistical'' and ``systematic''.
And what's ``systematic'' to some users will be ``statistical'' to others.
We will return to that point below.

One of my grad-school mentors\footnote{This was Gerry Neugebauer (1932--2014).
  He was one of the pioneers of infrared astrophysics, and the US
  lead of the NASA \textsl{IRAS} Mission, and a wonderful human
  being.} liked to say, when I said something about ``errors'' or
``error bars'', that ``they are \emph{uncertainties} not
\emph{errors}! If they were \emph{errors}, we'd correct them!'' That
phrase rings in my head every day. But this points out a difficulty,
which is in the interpretation of uncertainty estimates. When we say
that some parameter is measured to be $42\pm 6$, what does that ``$\pm 6$''
mean?  The answer depends a bit on your statistical philosophy:
If you are a Bayesian, it means that you believe that, with some
fairly well-defined probability (like maybe 68\,percent), the
parameter is within that range.

Seem straightforward? It is, but the straightforwardness of the Bayesian
comes at a cost, which is in assumptions. If you don't want to make some
of those assumptions, you can be a frequentist instead, but then the
interpretation of the measurement $42\pm 6$ is not so simple! For a frequentist
the meaning is that, if the \emph{true}
value of the parameter were $42$,
in some well defined fraction (like maybe 68\,percent)
of hypothetically repeated experiments with similar-quality data the
best-fit or estimated value of the parameter would be within that range.
That is definitely \emph{not} straightforward.
The Bayesian's answer is about the \emph{parameter}, the frequentist's
answer is about hypothetically repeated \emph{counterfactual experiments}.
The trade-off between assumptions made and
simplicity of interpetation will come up again below.
And, combined with this, there are things to say about the terribly named
``confidence interval'' and the even worse-named ``credible region'' and other
abominations.

Most of you, I hope, don't care about such pedantic details and just
want an \emph{uncertainty estimate on your measurement}. In what follows,
I will try to help you get that, with enough discussion to answer questions
and explain your choices. We'll start with the theoretical approaches and
then move on to the empirical approaches, followed by some troubleshooting and
discussion about communication and assessment.

\section{Simplest case: the linear least-square fit}

The simplest case for uncertainty estimation---and a case you may have
encountered previously---is a standard, linear fit, as we have
discussed endlessly in this series.\footnote{See, for example, the
  first two \sectionname s of \cite{straightline}. And also the
  applied math in \cite{gaussianproduct}.}  This is the case in which
the following things hold:
\begin{itemize}
\item You have $N$ data points $y_n$. These could be scalars or vectors, but
  let's call them scalars for now.
\item Each data point $y_n$ has an associated noise variance
  $\sigma^2_n$, which is the variance of zero-mean Gaussian noise that
  affects each data point $y_n$.  You believe these noise variance
  estimates.\footnote{We will return to the question of testing or
    believing your uncertainties below. But one thing to note right here
    is that there are many scientific contexts in which you do not have any
    good idea of these individual-datum uncertainty variances $\sigma^2_n$.
    If you don't know these, you can't use the uncertainty estimate developed in
    this section and you need to use empirical methods like those discussed in
    \sectionname~\ref{sec:empirical}. That is, this assumption---that you have
    some quantitative understanding of the noise in your data---is critical
    to the theoretical or information-theoretic discussion in this \sectionname.}
\item There are no other sources of noise and there is no
  non-Gaussianity (no skewness or kurtosis or outliers or the like) to
  the noise.
\item The noise in data point $y_n$ is independent of the noise
  in any other data point.\footnote{This assumption turns out to be the
    easiest---of all these assumptions---to relax.}
\item There is a set of $K$ features $x_{kn}$ for each data point $y_n$ such that
  the expectation for data point $y_n$ is going to be fit as a linear combination
  of these features. That is, the model\footnote{I use the word ``model'' to mean
    a collection of things. It is (at least) a prediction for the data and a
    prediction for the distribution of the noise. A model, for me, is everything
    that you need to construct a likelihood function, or a pdf for the data given
    your parameters.}
  is
  \begin{equation}
    y_n = \sum_{k=1}^K a_k\,x_{kn} + \mbox{noise}
    \quad,
  \end{equation}
  where the noise on data point $y_n$ is drawn, by assumption, from a distribution
  that is Gaussian, zero mean, and variance $\sigma^2_n$.
\end{itemize}
When \emph{all} of these assumptions hold---or, really, when you are
willing to assume \emph{all} of these things---the maximum-likelihood
estimates\footnote{I will distinguish a parameter,
  for example $a_2$, from an \emph{estimate} of that parameter, for example $\hat{a}_2$.
  The estimate is the outcome of applying an estimator to the data.
  The parameter can in principle take on values other than the estimated value; the
  parameter is a variable whereas the estimate is a value for that variable.}
$\hat{a}_k$ for the parameters $a_k$ can be found by minimizing what
physicists call ``chi-squared''\footnote{For physicists, chi-squared is a statistic
  of the data or an objective function to be minimized. It is like a metric distance
  between the data and the model, where the metric is the inverse variance tensor
  of the data. For statisticians, chi-squared is a distribution. It is the distribution
  you expect for the sum of squares of (zero-mean) Gaussian draws. This difference in
  the meaning of the word ``chi-squared'' causes some
  confusion when we participate in interdisciplinary activities in data analysis.}
\begin{equation}
  \chi^2 = \sum_{n=1}^N \frac{\left[y_n - \sum_k a_k\,x_{kn}\right]^2}{\sigma^2_n}
\end{equation}
The magic\footnote{This magic is described in much more detail in \cite{straightline}.}
is that the maximum-likelihood parameter estimates
$\hat{a}_k$ are given by straightforward linear algebra:\footnote{It is beyond the
  scope of this \documentname, but if you are actually implementing the estimator
  of \equationname~(\ref{eq:llsf}), you don't want to naively construct the
  tensor $\tC$ (which is almost entirely zeros). Write your code to just do the
  diagonal multiply directly. Also, you never want to use the inverse function
  \code{inv()}; instead use the \code{solve()} or \code{lstsq()} function, which
  does the multiply by the inverse more precisely than just naively taking the
  inverse. There are a lot more things to say about implementation that go
  beyond these pieces of advice; I am putting these here just to make the point
  that implementation of the simple formula isn't, itself, necessarily simple.}
\begin{equation}\label{eq:llsf}
  \hat{\va} = [\mX\T\cdot\tC\inv\cdot\mX]\inv\cdot\mX\T\cdot\tC\inv\cdot\vy
\end{equation}
\begin{equation}
  \hat{\va}\T \equiv \begin{bmatrix} \hat{a}_1 & \hat{a}_2 & \hdots & \hat{a}_K \end{bmatrix}
\end{equation}
\begin{equation}
  \mX \equiv \begin{bmatrix}
    x_{11} & x_{21} & \hdots & x_{K1} \\
    x_{12} & x_{22} & \hdots & x_{K2} \\
    x_{13} & x_{23} & \hdots & x_{K3} \\
    \vdots & \vdots &        & \vdots \\
    x_{1N} & x_{2N} & \hdots & x_{KN}
    \end{bmatrix}
\end{equation}
\begin{equation}
  \tC\inv \equiv \begin{bmatrix}
    1/\sigma^2_1 & 0 & 0 & \hdots & 0 \\
    0 & 1/\sigma^2_2 & 0 & \hdots & 0 \\
    0 & 0 & 1/\sigma^2_3 & \hdots & 0 \\
    \vdots & \vdots & \vdots & & \vdots \\
    0 & 0 & 0 & \hdots & 1/\sigma^2_N \\
    \end{bmatrix}
\end{equation}
\begin{equation}
  \vy\T \equiv \begin{bmatrix} y_1 & y_2 & y_3 & \hdots & y_N \end{bmatrix}
  \quad ,
\end{equation}
where $\hat{\va}$ is the (column) vector of parameter estimates $\hat{a}_k$,
$\mX$ is the design matrix of features $x_{kn}$,
$\tC\inv$ is the inverse variance tensor for the data\footnote{Note about
  typesetting: I typeset vectors---which are all column vectors---like
  $\va$, $\vy$. I typeset arbitrary rectangular matrices like $\mX$
  and variance tensors or information tensors (which must be
  non-negative definite, square martrices) like $\tC$.}
which is diagonal in this simple case),
and
$\vy$ is the (column) vector of data points $y_n$.

When this (\ref{eq:llsf}) is your estimator for $\hat{\va}$, and when
the assumptions (listed above) hold (or you are willing to hold
them\footnote{One of the themes of this \documentname\ and the others in this
  series is that data analyses are always technically \emph{subjective}: You
  make choices, based on your beliefs, and your methods flow from your choices.
  Your estimator and your uncertainty on your estimated values both are
  included in this: You have to make strong assumptions, hopefully based on your
  beliefs, and maybe also on your \emph{utility} (because there are trade offs
  between being accurate with your data and being tractable in your code).
  I don't think this subjectivity is a problem, but I do think that it has to be
  acknowledged and addressed. One of the most important things to note here is
  that although things like Gaussianity and independence are \emph{assumptions},
  these assumptions can be checked, empirically, with statistical tests. That is,
  assumptions are subjective and defining, but they are not outside the realm
  of empirical science. They are part of it.}),
then the formal or standard uncertainty on the $K$-vector of parameter
estimates $\hat{\va}$ is the $K\times K$ uncertainty tensor $\tC_a$
\begin{equation}
  \tC_a = [\mX\T\cdot\tC\inv\cdot\mX]\inv
  \quad.
\end{equation}
How do we interpret this object?

...HOGG interpret this object.

...If you want the uncertainty on JUST ONE component of $\va$, profiling out
the rest or marginalizing out the rest (with a wide prior) then the relevant
uncertainty is the diagonal element of $[\mX\T\cdot\tC\inv\cdot\mX]\inv$. There
is a big difference between the diagonal element of $\tC_a$ and the inverse
of the diagonal element of $\tC_a\inv$.

At this point we have delivered what is requested: An uncertainty estimate
on the measurement!
If this serves your needs, you are done and you don't need to read further,
unless you want some understanding of what just happened.
Of course for many of you I expect that this does \emph{not} meet your needs:
It might be that your model is nonlinear, so the optimum (the maximum-likelihood
estimator) is not a linear function of the data. That case will be discussed
in \sectionname~\ref{sec:nonlinear}.
It might be that you don't have an executable or tractable likelihood function.
In this case, you either have to use data simulation to get your uncertainties,
as discussed in \sectionname~\ref{sec:simulation}, or empirical approaches
like bootstrap or jackknife, as discussed in \sectionname~\ref{sec:empirical}.
It might be that I haven't met your needs because you don't have good noise
estimates for your data points, or you aren't using a maximum-likelihood method
at all. Or you don't have a generative model at all. Or you don't believe it, or
want to check it. In these cases, you must use empirical approaches like those discussed
in \sectionname~\ref{sec:empirical}. You have no choice.
And of course I might be missing the point because your analysis is Bayesian.
In that case I will talk to you in \sectionname~\ref{sec:bayes}.

Perhaps because I am a physicist, I like to think in terms of units and dimensions.
The linear-algebra objects $\vy$, $\tC$, and $\mX$ have units,
and the result $\hat{\va}$ must also have correct units.
The $N$-vector of data $\vy$ has, say,
``data units'' (maybe photons per second, say, or maybe Kelvins, or maybe
magnitudes or Janskys if you are an astronomer).
Or maybe even each row of $\vy$ has its own special units, if the data are
very heterogeneous.
The inverse variance tensor $\tC\inv$ has inverse-data-squared units, because
it contains inverted, squared uncertainties.
The design matrix $\mX$ has units of ``data per parameter'' because when the
features $x_{kn}$ in the design matrix are multiplied by the parameters $a_k$
in the parameter vector $\va$ they produce predictions for the data.
Again, every row of the parameter vector $\va$ can have different units, but
then every element $x_{kn}$ of the design matrix $\mX$ has units of data element
$y_n$ divided by the units of parameter $a_k$.
Indeed, you can think of $\mX$ as a derivative of the expectation of the data
vector $\vy$ with respect to the parameter vector $\va$
\begin{equation}
  \mX \approx \frac{\dd\vy}{\dd\va}
\end{equation}
where I am being a bit loose with notation because really it is not the
derivative of the data $\vy$ but the derivative of the \emph{expectation} of the
data.
With these observations, it is a nice exercise to show that the estimator
$\hat{\va}$ given in \equationname~(\ref{eq:llsf}) has correct parameter
units, and so does the uncertainty tensor $\tC_a$.\footnote{The
  requirement that equations have correct units---that the left-hand
  and right-hand sides of every equation have the same units---is a
  fundamental symmetry of everything in all of science. Yes, I think
  this is a symmetry.}

It is sometimes useful for intuition-building to note that the uncertainty
tensor $\tC_a$ shares things in common with the kinds of uncertainty or noise
propagation that is sometimes taught in undergraduate lab classes. In these
contexts you learn.... HOGG!

Finally, we should end this \sectionname\ on an upsetting note:
The parameter uncertainty variance estimate
$\tC_a = [\mX\T\cdot\tC\inv\cdot\mX]\inv$
is based on the design matrix $\mX$ and the noise variance for the data $\tC$,
but it makes no reference to the data $\vy$ themselves!
That is, the data don't enter, only the precision of the data enters.
That is, the uncertainty variance could be computed on the basis of the
expected variance without any idea about the expected data.
Or even the parameter values.
See how strange this is?
This is one of the indicators that the standard uncertainty estimate is a
very theoretical object, and very brittle. We'll say more about this below.
But first let's see how this uncertainty estimate generalizes and is justified.

HOGG: EXERCISE HERE where we do repeated experiments generating data
and doing a linear fit and show that the best-fit parameters are
Gaussian-distributed according to the rules above.

\section{Generalization to nonlinear models}\label{sec:nonlinear}

If you want to derive the magical linear least-squares estimator above,
or if you want to generalize what we have done to nonlinear models,
it is useful to note that the linear-algebra objects defined
above have the following properties:
The objective function $\chi^2$ can be written as
\begin{equation}
  \chi^2(\va) = [\vy - \mX\cdot\va]\T\cdot\tC\inv\cdot[\vy - \mX\cdot\va]
  \quad,
\end{equation}
where we have written this as a function of the parameter vector $\va$ because
that is the thing with respect to which we minimized to get the estimates
$\hat{\va}$.
The likelihood function---the probability density function (pdf) for
the data given the parameters---can be written as
\begin{equation}
  p(\vy\given\va) = \normal(\vy\given\mX\cdot\va,\tC)
  \quad,
\end{equation}
where $\normal(x\given m,V)$ is the Gaussian pdf given mean $m$ and variance $V$.
The log-likelihood function is
\begin{equation}
  \ln\like(\va) = -\frac{1}{2}\,\chi^2(\va) -\frac{1}{2}\,||2\pi\,\tC||
  \quad,
\end{equation}
where $||V||$ is the determinant operator and we have written the
log-likelihood as a function of the parameters $\va$ even though it is
also a function of the data $\vy$ and the noise variance $\tC$.
We write it as a function only of the parameters because when we manipulate it
(in a moment), we only take derivatives with respect to these parameters.

If we take one derivative of $\ln\like(\va)$ with respect to $\va$ and set it
to zero, we find the optimum given by the magic formula in
\equationname~(\ref{eq:llsf}).
If we take two derivatives of $\ln\like(\va)$ with respect to $\va$ we get
\begin{equation}
  -\frac{\dd^2}{\dd\va^2}\,\ln\like(\va) = \mX\T\cdot\tC\inv\cdot\mX = \tC_a\inv
  \quad.
\end{equation}
That is, the (negative of the) second derivative of the log-likelihood function is
the inverse variance tensor $\tC_a\inv$.
That's important for generalizing the simple, linear case.

...Comment on units and shapes of objects.

...Non-linear fit. What's different?

...The concept of linearizing and then doing what you did above. Note that this
requires a choice of a fiducial point around which to linearize.

...A reminder that uncertainties are rarely precisely known, so often linearization
is as good as you need; do it and publish and stop reading now.

If you want more theoretical backing, read \sectionname~\ref{sec:info}. If you
don't trust your likelihood function, head to \sectionname~\ref{sec:empirical}.
If you not only believe your likelihood function, but you are happy to put priors
on all your parameters---or if you want to run MCMC---check
out \sectionname~\ref{sec:bayes}.

\section{Information theory}\label{sec:info}

...NOTE language that the inverse variance is called the information tensor or the precision matrix or other such things.

...Connection to Cram\'er--Rao bound and Fisher information.

Relationship of first derivative squared to second derivative.

Note inherent frequentism in this point of view.

...Come back to inverse-variance weighting. Note that this appears everywhere
in the linear case. That's deep information-theory shih.

\section{Bayes}\label{sec:bayes}

If your data analysis is Bayesian,\footnote{There are a lot of things that
  can be meant by the word ``Bayesian'' but for our purposes here it only means
  that you are doing your data analysis with a likelihood function \emph{and} a
  prior pdf over the parameters. The information-theoretic considerations in the
  previous \sectionname s are explicitly frequentist in justification, but (as so
  often happens in data analysis) the final results are very similar between
  frequentist and Bayesian uncertainty estimates. It is mainly their
  interpretations that differ. Not all statisticians would agree with me
  on these points.}
and you are obeying the rules of
Bayesian inference, then the uncertainty estimation and noise propagation
are automatically built in to your analysis.
That is, in Bayesian inference, you are required to have a likelihood
function\footnote{Of course there are Bayesian likelihood-free methods
  such as (the terribly named) Approximate Bayesian Computation (see, for
  example, \cite{??}). These methods are data-simulation methods and
  live somewhere between the things I'm writing about in this \sectionname\ and
  the things in \sectionname~\ref{sec:simulation}.} and also a prior, and your
parameter estimation or measurement is performed by multiplying these
two and displaying or summarizing the posterior pdf for the parameters
of interest.
Provided that you believe your likelihood function (which, if you are
a subjective Bayesian, you explicitly must) and you believe your prior
pdf (same), then you don't have to do anything else at all: Your posterior
pdf is literally a probability density over all possible parameters that
are consistent with your prior and your data. It contains all of the
measurement and uncertainty that you could possibly want or need to know.
You have no choice, and you get a full answer.

...All that said ... So you better believe that model really really well...
Or make it more baroque. (Good option for a problem / exercise!)

...Now consider the fully Gaussian linear model again and note that the
Bayesian uncertainty has closed form and is incredibly similar to the frequentist
result; the only difference is that the prior pdf appears like an additional
data point. And there is an additional---critical---assumption.

...Now consider the linearized non-linear case. Same thing: very similar to
the frequentist result.

...In general...it's a mess but you get to use MCMC (note and cite). MCMC is a
method of last resort but it requires little thinking (note on climate impact).

...Note that you can't run MCMC without priors. And no, flat priors are not no
priprs. A sampling \emph{has no meaning} without priors. Why not?

...And the theorem that Goodman pointed me to that give some limits in which
Bayesian and frequentist confidence intervals are identical.

...Now am I bringing together frequentists and Bayesians? No, because the
interpretation is \emph{totally different}. In the one you have figured out the
probability in which your experiment would get your data, given a particular
parameter value; while in the other you have figured out the probability that
the universe has your parameter value, given your data. So be very careful
what you say in your papers, people. Take some time to criticize but explain the
confidence vs credible terminology.

...But I will say something that both frequentists and Bayesians can agree on:
Lots of investigators vary the priors, but it is the likelihood function
that needs to be challenged, usually! Cite Gelman.

\section{Data simulation}\label{sec:simulation}

Everything in the preceding \sectionname s has assumed that you are in posession
of an executable likelihood function, or probability (or really pdf)
for your data given the parameters.
In many cases you don't have such a function, either because you don't have
a well-formed belief about how your data are generated (or the noise process
in particular), or because you do, but you can't tractably compute the
pdf for the data evaluated at the parameters.
This situation is not uncommon; we encounter it, for example,
when we measure large-scale structure (two-point and higher-order statistics)
in cosmology.
When you don't have a tractable likelihood function, you have two-ish choices.
One is to perform data simulations, which is the subject of this \sectionname,
and the other is to use empirical measures of the uncertainty like bootstrap and
jackknife, which are the subject of \sectionname~\ref{sec:empirical}.

...What to say here?

...How many simulations do you need?

...Different kinds of parameters and resimulation?

...Emulators?

...What do you want to compute, really? Come back to the mistakes of LSS.

\section{Jackknife and bootstrap}\label{sec:empirical}

In many---or even most---of the data analysis projects in which I have been
involved, we have \emph{not} confidently believed all of our assumptions about the
noise processes contributing to our data.
That is, we have not been confident that we have a realistic model of how
the data were generated.
This means that even if we can compute a likelihood function, we don't entirely
believe it.
And even if we can simulate our data, we don't believe that the simulated data
will have all the issues that beset our real data.
We cry out for an empirical method to assess our uncertainties!

The statistics community hears our cries, and has responded with many options.
Here we will discuss two that can be used (with care, perhaps) in almost every
setting I regularly encounter: Jackknife and bootstrap.

...Remind the reader that we didn't use the agreement between model and data
at all in the above!!

...Remind the reader that if our data is very noisy, the scatter around the
model prediction or expectation will be large. So the noise variance is empirically
accessible.

...etc.

...At the end come back to the point that even if you DO believe your noise
model and your LF you still want to do jackknife to check it all. This test
MUST PASS.

\section{Nuisance parameters}

Difference between marginalizing and profiling.

Identicality when it comes to the linear, Gaussian case.

What it looks like.

What it looks like in very nonlinear situations (like period fitting).

\section{Systematic error and theoretical uncertainty}

There is only bias and variance; nothing else.

What is a statistical uncertainty? And what is a systematic one?

When do you want to use the former? When do you want to add both in quadrature?

\section{Visualizing and reporting uncertainties}

error bars

68, 95, 99.whatever and all that.

mean, variance, skewness, kurtosis and all that.

asymmetric erro bars.

violin plots.

banana plots.

corner plots.

catalogs and data releases. Cite Malz.

\section{Other and strange situations}

Upper limits and how to use them. And why NOT to report them. What are
you trying to report?

What if your uncertainties are part of the model? This is possibly
true if you are using a GP for your noise, say. Or if your
uncertainties depend on your model parameters, as they do in a Poisson
process. Nothing really changes, but say words.

\section{Common mistakes and troubleshooting}

My measurement is ridiculously precise! This makes no sense!

About 68\,percent of your values should be outside one sigma of the
mean prediction (expectation) of your model. What do think or do if
that's not true?

The uncertainty on the mean vs the distribution of values.

My data aren't well fit by my model; what does that mean for my uncertainty
analysis?

Do I multiply my uncertainties up or do I add something in quadrature?

Do I multiply my uncertainties down?

Sometimes you only need the error bars to be correct in a
\emph{relative} sense. If, say they are just being used to weight (by
their squared inverses) data in a fit.

Sometimes you have huge theoretical uncertainties and you want to use
them in your analysis. Sometimes you have these and you \emph{don't} want
to use them in your analysis.

I have a good LF but then I did jackknife and it disagrees!

\section{Discussion}

Hello world.

% Render the references
\clearpage\raggedright
\bibliography{refs}

\end{document}
